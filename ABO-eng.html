<!DOCTYPE html>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
  <title>ABO Image classification</title>
  <meta charset="utf-8" />
  <meta http-equiv="Permissions-Policy" content="camera=(self)">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" charset="UTF-8" />
  <link rel="stylesheet" href="assets/css/main.css" />
  <noscript>
    <link rel="stylesheet" href="assets/css/noscript.css" />
  </noscript>
  <link rel="icon" href="images/icon.webp" type="image/x-icon" />
  <script>
    function showMoreCategories() {
      const groups = document.querySelectorAll('.category-group');
      groups.forEach((group, index) => {
        if (index !== 0) {
          group.style.display = 'block';
        }
      });
      document.getElementById('show-more-btn').style.display = 'none';
      document.getElementById('show-more-btn-2').style.display = 'none';
    }
  </script>
  <script>
    function toggleVersion() {
      const resumed = document.querySelectorAll('.resumed');
      const extended = document.querySelectorAll('.extended');

      resumed.forEach((section) => {
        if (section.style.display === 'none' || window.getComputedStyle(section).display === 'none') {
          section.style.display = 'block';
          document.getElementById('complete-btn').style.display = 'none';
          document.getElementById('resumed-btn').style.display = 'flex';
        } else {
          section.style.display = 'none';
        }
      });

      extended.forEach((section) => {
        if (section.style.display === 'none' || window.getComputedStyle(section).display === 'none') {
          section.style.display = 'block';
          document.getElementById('resumed-btn').style.display = 'none';
          document.getElementById('complete-btn').style.display = 'flex';
        } else {
          section.style.display = 'none';
        }
      });
    }
  </script>

  <script>
    function showMoreStats() {
      const groups = document.querySelectorAll('.cat_stats');
      groups.forEach((group, index) => {
        if (index !== 0) {
          group.style.display = 'table-row';
        }
      });
      document.getElementById('show-more-table-btn').style.display = 'none';
    }
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      const progressBars = document.querySelectorAll(".progress-bar");

      // IntersectionObserver callback function
      const animateProgressBar = (entries, observer) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const progressBar = entry.target;
            const value = parseFloat(progressBar.getAttribute("value"));
            progressBar.setAttribute("value", "0"); // Start from 0

            // Animate the progress bar value
            let currentValue = 0;
            const increment = value / 100; // Adjust this for speed
            const interval = setInterval(() => {
              currentValue += increment;
              if (currentValue >= value) {
                currentValue = value;
                clearInterval(interval);
              }
              progressBar.setAttribute("value", currentValue);
            }, 15); // Adjust this for smoothness

            // Stop observing the progress bar after animation is done
            observer.unobserve(progressBar);
          }
        });
      };

      // Create an IntersectionObserver
      const observer = new IntersectionObserver(animateProgressBar, {
        threshold: 0.1 // Trigger when 10% of the element is visible
      });

      // Observe each progress bar
      progressBars.forEach(bar => observer.observe(bar));
    });
  </script>

  <style>
    .resumed {
      display: none
    }

    html {
      scroll-behavior: smooth;
    }

    h3 {
      color: #ee5f0f;
    }

    h4 {
      font-size: 0.9em;
    }

    h5 {
      font-size: 0.8em;
    }

    .white-big {
      font-size: 1.3em;
      text-align: center;
      margin-bottom: 20px;
      border-bottom: none !important;
    }

    .orange_title {
      font-size: 1.1em;
    }

    .variable {
      /* font-family: "Courier New", Courier, monospace; */
      background-color: #212430;
      padding: 2px 4px;
      color: #ee5f0f;
      font-size: 12px;
      font-weight: bold;
    }

    .variable_2 {
      /* font-family: "Courier New", Courier, monospace; */
      background-color: #212430;
      padding: 2px 4px;
      color: #ee5f0f;
      font-size: 16px;
      font-weight: bold;
    }

    .main-image {
      margin-right: 20px;
      height: auto;
      width: 100px;
      /* Maintain aspect ratio */
      object-fit: cover;
      /* Adjust the margin value as needed */
    }

    .oth-image {
      margin-right: 20px;
      height: auto;
      width: 100px;
      /* Maintain aspect ratio */
      object-fit: cover;
      /* Adjust the margin value as needed */
    }

    .gtr-uniform {
      display: flex;
      align-items: center;
      gap: 10px;
      /* Optional: Adds a gap between all columns */
    }

    .image-fit {
      width: 100%;
      max-width: 300px;
      height: auto;
      object-fit: cover;
    }

    /* Form Container */
    #contact-form {
      background-color: #2e3141;
      padding: 40px;
      border-radius: 8px;
    }

    /* Form Fields */
    .fields {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
    }

    .field {
      width: 100%;
    }

    /* Half-width fields for larger screens */
    .field.half {
      width: calc(50% - 10px);
    }

    .field.half.second {
      width: calc(50% - 10px);
      padding-right: 20px;
      padding-left: 0px;
    }

    /* Input and Textarea Styling */
    input[type="text"],
    input[type="email"],
    textarea {
      width: 100%;
      padding: 12px 15px;
      border: 1px solid #ccc;
      border-radius: 4px;
      background-color: #fff;
      color: #333;
      font-size: 16px;
    }

    /* Button Styling */
    .button {
      background-color: #ee5f0f;
      color: #fff;
      padding: 12px 30px;
      border: none;
      cursor: pointer;
      border-radius: 4px;
      font-size: 16px;
      text-align: center;
      display: block;
      margin: 0 auto;
      transition: background-color 0.3s;
      line-height: 0;
    }

    .button:hover {
      background-color: #ff7043;
    }

    /* Labels */
    label {
      color: #f0f0f0;
      font-size: 14px;
      margin-bottom: 8px;
      display: block;
    }

    /* Form Responsiveness */
    @media (max-width: 768px) {
      .field.half {
        width: 100%;
      }
    }

    /* Styling for the home images container */
    .home-images {
      display: flex;
      justify-content: space-between;
      background-color: #3a3d52;
      /* Background color for the row */
      padding: 20px;
      border-radius: 8px;
    }

    /* Styling for each image container */
    .home-image {
      flex: 1;
      margin: 0 10px;
      background-color: #4b4e63;
      /* Default background color for images */
      padding: 10px;
      border-radius: 8px;
      text-align: center;
    }

    /* Styling for the main image */
    .home-image.main-image {
      background-color: #5c5f75;
      max-width: 207px;
      /* Slightly different background color */
    }

    /* Styling for the images inside the containers */
    .home-image img {
      max-width: 100%;
      height: auto;
      border-radius: 4px;
    }

    .oth-row {
      display: flex;
      gap: 10px
    }

    .generic-cat {
      display: flex;
      flex-direction: row;
    }

    .gradio {
      width: 100%;
      height: auto;
      min-height: 483px;
    }

    /* Responsive adjustments */
    @media (max-width: 768px) {
      .home-images {
        flex-direction: column;
        gap: 20px;
      }

      .generic-cat {
        display: flex;
        flex-direction: column;
      }

      .home-image {
        margin: 10px 0;
      }

    }

    /* Responsive adjustments */
    @media (max-width: 878px) {

      .gradio {
        width: 100%;
        height: auto;
        min-height: 838px;
      }
    }

    /* Progress Bar Styling */
    .progress-bar {
      width: 100%;
      height: 8px;
      appearance: none;
      -webkit-appearance: none;
      margin-bottom: 5px;
    }

    /* Custom styles for different browsers */
    .progress-bar::-webkit-progress-bar {
      background-color: #f0f0f0;
      border-radius: 4px;
    }

    .progress-bar::-webkit-progress-value {
      background-color: #fe8d59;
      border-radius: 4px;
    }

    .progress-bar::-moz-progress-bar {
      background-color: #fe8d59;
      border-radius: 4px;
    }

    .category-group {
      margin-bottom: 20px;
    }

    .category-group-2 {
      margin-bottom: 20px;
    }

    .cat_stats {
      display: none;
    }

    #show-more-btn {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 11px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
    }

    #show-more-btn-2 {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 11px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
    }

    #show-more-table-btn {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #complete-btn {
      display: flex;
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #resumed-btn {
      display: none;
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #show-more-btn:hover {
      background-color: #ff7043;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-family: Arial, sans-serif;
    }

    th,
    td {
      padding: 10px;
      text-align: center;
      border: 2px solid #666;
      font-weight: 600;
      /* Make text slightly bolder */
    }

    th {
      background-color: #2e3141;
      color: #ee5f0f;
      border-bottom: 3px solid #ee5f0f;
    }


    td.boolean-cell {
      font-size: 1.5em;
    }

    td.accuracy,
    td.loss,
    td.val_accuracy,
    td.val_loss {
      width: 70px;
      color: #fff;
      /* Ensure text contrast on colored background */
    }

    .green-check {
      color: #66bb6a;
    }

    .red-cross {
      color: #ff7043;
    }

    .separator {
      border: none;
      width: 1px;
    }

    td.separator {
      background-color: #2e3141;
      /* Set a consistent color for the separator column */
      border: none;
      /* Remove the border for the separator column */
    }

    tr:nth-child(odd) {
      background-color: #3a3b44;
      /* Softer color for odd rows */
    }

    tr:nth-child(even) {
      background-color: #404351;
      /* Slightly darker, but still subtle */
    }

    th,
    td {
      padding: 5px;
      /* Reduce padding to make rows less tall */
      text-align: center;
      border: 2px solid #666;
      font-weight: 600;
      /* Keep the bold text */
    }

    td.accuracy,
    td.loss,
    td.val_accuracy,
    td.val_loss {
      padding: 8px;
      /* Slightly larger padding for the score cells for readability */
    }

    tbody tr {
      height: 40px;
      /* Adjust this value to your preferred fixed row height */
    }

    td {
      vertical-align: middle;
      overflow: hidden;
      /* Prevent overflow from expanding the row */
      white-space: nowrap;
      /* Prevent text wrapping inside the cells */
      text-overflow: ellipsis;
      /* Add ellipsis if the text is too long */
    }

    table td {
      padding: 0em 0.35em;
    }

    table th {
      padding: 5px;
    }

    .a-custom {
      color: #ee5f0f;
      text-decoration: underline;
      cursor: pointer;
    }


    .tooltip-container {
      position: absolute;
      display: inline-block;
      cursor: pointer;
      right: 8px;
      top: 4px;
    }

    .tooltip-container .tooltiptext {
      visibility: hidden;
      width: 150px;
      background-color: #555;
      color: #fff;
      text-align: center;
      border-radius: 5px;
      padding: 5px;
      position: absolute;
      z-index: 1;
      bottom: 125%;
      /* Position above the icon */
      left: 50%;
      margin-left: -75px;
      opacity: 0;
      transition: opacity 0.3s;
      width: 200px;
    }

    .tooltip-container .tooltiptext::after {
      content: "";
      position: absolute;
      top: 100%;
      /* Arrow below the tooltip */
      left: 50%;
      margin-left: -5px;
      border-width: 5px;
      border-style: solid;
      border-color: #555 transparent transparent transparent;
    }

    .tooltip-container:hover .tooltiptext {
      visibility: visible;
      opacity: 1;
    }

    .info-icon {
      margin-left: 5px;
      color: #ee5f0f;
      font-weight: bold;
    }
  </style>
</head>

<body class="is-preload" style="
      background-image: linear-gradient(
          to top,
          rgba(46, 49, 65, 0.8),
          rgba(46, 49, 65, 0.8)
        ),
        url(images/ABO/cosas.webp);
    ">

  <!-- Disclaimer: El objetivo de este blog es demostrar mis habilidades en machine learning, porque mis habilidades como web developer no me enorgullecen 🤦‍♂️ -->

  <!-- Advertencia: Este código puede generar secuelas como falta de apetito, dolor de ojos, depresión y pérdida de cabello. Continúa leyendolo bajo tu propio riesgo -->


  <!-- Page Wrapper -->
  <div id="page-wrapper">
    <!-- Header -->
    <header id="header">
      <a href="index-eng.html">
        <h1>Nicolás Pavón</h1>
      </a>
      <nav>
        <a href="ABO.html">ESP</a>
        <a href="#menu">Menu</a>
      </nav>
    </header>

    <!-- Menu -->
    <nav id="menu"></nav>

    <!-- Wrapper -->
    <section id="wrapper">
      <header id="ABO-header">
        <div class="inner">
          <h2>Classifying Amazon images</h2>
        </div>
      </header>

      <!-- Content -->
      <div class="wrapper">
        <div class="inner">
          <section>
            <h3 class="orange_title major">Introduction</h3>
            <p>This is the continuation of the project on neural networks and image classification from the course
              "Artificial Intelligence 2" taught by Juan Kurucz and Ernesto Ocampo at the Catholic University of
              Uruguay. It is about training a neural network to classify images from an e-commerce product dataset,
              where i deal with dataset balancing issues and many other problems with some cool techniques.</p>
            <p>In this blog, I explain in detail the problems and solutions encountered along the way, which makes the
              full version quite extensive (15 minutes of reading). If you are short on time, you can switch to the
              summarized version (8 minutes) by clicking the button below:</p>


            <button id="complete-btn" onclick="toggleVersion()" style="margin-top: -20px; margin-bottom: 60px">Show
              resumed version</button>
            <button id="resumed-btn" onclick="toggleVersion()" style="margin-top: -20px; margin-bottom: 60px">Show
              original version (recommended)</button>
          </section>
          <section class="extended">
            <h3 class="orange_title major">The issue</h3>
            <p>
              In the last 20 years, e-commerce has grown exponentially. Just look at the power and size of sites like
              Amazon, Alibaba, or even Mercado Libre to realize how important they are today. From this, we can conclude
              that if there’s something these tech giants have, it's a massive amount of data, including many images.
              However, data is useless if it cannot be interpreted and worked with, so it's helpful to classify it to
              make proper use of it and maximize its potential.
            </p>
            <p>However, my goal is not to solve this huge global challenge. To be realistic, I’m more interested in
              showcasing my machine learning skills 😎. So, here we go!
            </p>


            <h3 class="orange_title major">The data</h3>
            <p>
              Among all the available datasets, we came across an <i>interesting</i> one, the <a class="a-custom"
                href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Amazon Berkeley Objects (ABO)
                Dataset.</a> This dataset provides images of approximately 147,000 Amazon products, along with
              corresponding metadata, including category, color, keywords, brand, name, model, among others. It also
              offers 3D renders and some other interesting details. While we have yet to explore the data in depth to
              determine its purity, we appreciate the fact that it comes from Amazon, making the images ideal for this
              problem.
            </p>


            <h3 class="orange_title major">The technology</h3>
            <p>
              Although transformers are commonly used for this type of problem today, in this case, we will use
              convolutional networks, starting with a pre-trained model like Inception-v3 and applying transfer
              learning. We will remove the top classification layers and add new layers specialized for this task.
              Finally, we will apply fine-tuning to improve performance. The code was developed <a class="a-custom"
                target="_blank"
                href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj?usp=sharing">in Google
                Colab.</a>
            </p>

            <h3 class="orange_title major">The objective</h3>
            <p>
              Our goal is to classify the product in the image, assuming the image corresponds to an e-commerce product.
              Taking this into account and analyzing the dataset, we observed the <i class="variable">product_type</i>
              property, which has around 574 classes that vary in level of specificity, from "RING" to "BISS" (Business,
              Industrial, and Scientific Supplies). We will use this property from the dataset as the starting point to
              train our model.
            </p>

          </section>
          <section class="resumed">
            <h3 class="orange_title major">Resume</h3>
            <p>
              E-commerce has grown tremendously in recent decades, driven by giants like Amazon and Alibaba. With this
              growth, the amount of data has also increased, especially in the form of product images. To make the most
              of this data, it is essential to classify it properly. Although, to be realistic, in this case, I’m not
              here to save global commerce, but rather to show what I can do in machine learning 😅. We’ll be working
              with the <a class="a-custom" href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Amazon
                Berkeley Objects (ABO) Dataset.</a>, which contains images and metadata of over 147,000 Amazon products.
            </p>
            <p>
              To tackle this classification problem, we’ll use convolutional networks and transfer learning with a
              pre-trained model like Inception-v3. We will remove the original classification layers and replace them
              with custom layers to improve performance on the specific task of product classification, fine-tuning the
              model. The goal is to classify as many different product classes as possible based on the
              <i>product_type</i> property of the dataset, which ranges from highly specific categories like "RING" to
              broader ones like "BISS" (Business, Industrial, and Scientific Supplies).
            </p>

          </section>
        </div>
      </div>
      <div class="wrapper alt style4">
        <div class="inner">
          <section>
            <h2 class="white-big major">Dataset structure and info</h2>
            <h3 class="orange_title major">Dataset structure</h3>
            <p>
              <a class="a-custom" href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">The dataset</a>
              includes several files for download, of which we are interested in <i class="variable">listings.tar</i>
              (product listings and metadata) and <i class="variable">images-small.tar</i> (catalog of images resized to
              a maximum of 256 pixels).
            </p>
            <p class="extended">The <i class="variable">listings.tar</i> file contains 15 .json files, each with a list
              of objects, with each object representing an Amazon product. We will use a script to convert the relevant
              information from these objects into .csv files, making them easier to work with. The objects have a series
              of attributes, and we will be focusing on <b><i>item_id</i></b>, <b><i>product_type</i></b>,
              <b><i>main_image_id</i></b>, and <b><i>other_image_id</i></b>.
            </p>
            <h3 class="orange_title major">Dataset Attributes and Statistics</h3>
            <p>
              Once we have the initial .csv file, we proceed to observe the class distribution:
            </p>


            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS inicial desb.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Initial dataset</i></p>
              </span>
            </div>
            <p class="extended">As seen in the image, the dataset is highly imbalanced, with many examples for certain
              categories and almost none for others. Upon closer inspection, we see that there are 574 categories, of
              which 460 have fewer than 100 examples. This is problematic, as we need a good number of images per
              category to successfully identify these types of objects, and only 100 or fewer is not enough.</p>
            <p class="extended">
              To deal with this issue, we will initially work only with the categories that have more examples,
              balancing them to avoid bias between categories during training. Originally, we decided to work with 170
              categories that had at least 50 examples per category. This didn’t work, so we reduced the dataset to
              include only categories with at least 150 examples, with a cap of 400. This was a somewhat arbitrary
              decision, so if necessary, a better selection of categories and examples could be made.
            </p>

            <p class="resumed">
              The dataset is imbalanced, with 574 categories, of which 460 have fewer than 100 examples, making it
              difficult to correctly identify objects. To address this, categories with at least 150 examples and a
              maximum of 400 were selected. This selection was somewhat arbitrary and could be improved if needed.
            </p>
            <p style="margin-bottom: 10px;">Once the filters were applied, we can observe the statistics of the final
              dataset:
            </p>


            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS min 150 max 400.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Simplified dataset</i></p>
              </span>
            </div>
            <p>In this dataset, we have 80 categories, much better balanced than the 574 in the initial dataset. This
              will make the task easier, as the final neural network will be simpler to train and will have a
              significantly higher average number of examples per category.</p>


            <h3 class="orange_title major" id="inspeccion">Dataset inspection</h3>
            <p>
              In this step, we will analyze the previously refined dataset in search of potential obvious issues, among
              which we found:
            </p>
            <h5>Confusable categories:</h5>
            <p>These categories contain objects that are very similar to each other. In some cases, the only way to
              differentiate them is by reading the text on the product label. This is a problem, as it will be difficult
              for the neural network to learn the differences.</p>
            <ul>

              <li><i class="variable">ACCESORY</i> &#8596; <i class="variable">HAT</i></li>
              <li><i class="variable">STORAGE_HOOK</i> &#8596; <i class="variable">TOOLS</i></li>
              <li><i class="variable">NUTRITIONAL_SUPLEMENT</i> &#8596; <i class="variable">VITAMINS</i> &#8596; <i
                  class="variable">HEALTH_PERSONAL_CARE</i></li>
              <li><i class="variable">LUGGAGE</i> &#8596; <i class="variable">SUIT_CASE</i></li>
              <li><i class="variable">FINERING</i> &#8596; <i class="variable">RING</i></li>
              <li><i class="variable">FINENECKLACEBRACALETANKLET</i> &#8596; <i class="variable">NECKLACE</i></li>
              <li><i class="variable">FINEEARING</i> &#8596; <i class="variable">EARRING</i></li>
            </ul>
            <p class="extended">Except for the cases of 'fine x' ↔ 'x', we will initially keep these categories and
              observe if they indeed pose classification issues. For the 'fine x' cases, we will keep the non-"fine"
              ones, as they are more general and still preserve the overall form.</p>

            <h5>Generic categories: </h5>
            <p class="extended">These categories contain very diverse objects, making it harder to train the network to
              find shared patterns. If all the objects in a category vary in shape, there is no set of features or
              patterns that unify them, and the network will not be able to categorize them efficiently. This would only
              be possible if each subgroup of objects in this category had enough images, but with maybe only 90 out of
              400 images belonging to one of these objects, it will be very difficult to train. For this reason, these
              categories will be removed from the dataset.</p>
            <p class="resumed">These categories are too diverse, making it difficult for the network to find common
              patterns. Since there aren’t enough images per subgroup within each category, the network won’t be able to
              classify them efficiently, so they will be removed from the dataset.</p>

            <div class="generic-cat">
              <ul>
                <li><i class="variable">HOME</i></li>
                <li><i class="variable">WIRELESS_ACCESORY</i></li>
                <li><i class="variable">ACCESORY_OR_PART_OR_SUPPLY</i></li>
                <li><i class="variable">BABY_PRODUCT</i></li>
                <li><i class="variable">COMPUTER_ADDON</i></li>
                <li><i class="variable">GROCERY</i></li>
                <li><i class="variable">SPORTING_GOODS</i></li>
                <li><i class="variable">PANTRY</i></li>
                <li><i class="variable">KITCHEN</i></li>
                <li><i class="variable">JANITORY_SUPPLY</i></li>
                <li><i class="variable">HOMEFURNITURE_AND_DECOR</i></li>
                <li><i class="variable">HARDWARE</i></li>
              </ul>

              <div class="gtr-uniform home-images"
                style="flex-direction: column; align-items: flex-start; margin-bottom: 20px; margin-left: 16px; flex: 1; min-width: 0;">
                <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Examples of
                  <i class="variable">HOME</i> products
                </h5>
                <div style="flex-direction: row; display: flex;">
                  <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                    <div class="oth-row">
                      <span>
                        <img src="images/ABO/HOME_3.jpg" alt="Home product example 2" />
                      </span>
                      <span>
                        <img src="images/ABO/HOME_2.jpg" alt="Home product example 1" />
                      </span>
                      <span>
                        <img src="images/ABO/HOME_1.jpg" alt="Home product example 1" />
                      </span>
                    </div>
                    <p style="margin: 0px;"><i>(They don't look alike at all)</i></p>
                  </div>
                </div>
              </div>
            </div>

            </p>
            <h3 class="orange_title major">Dataset Balancing</h3>
            <p class="extended">
              As previously mentioned, a potential issue is the bias that can arise from imbalanced examples when
              training a neural network. If in our network we have a thousand examples of shoes and a hundred examples
              of sofas, for the network, the likelihood of receiving a shoe is 10 times higher than receiving a sofa. In
              this case, the network might always return "shoe," being correct most of the time. This would drastically
              affect classification, so we aim to keep the dataset as balanced as possible.

              In our case, we have several categories with fewer than 400 examples, which is the ideal number we want to
              maintain across all categories. To achieve the desired balance, we will also work with the
              "other_images" available for each object provided by the dataset. These images can help us complete the
              number of images for the categories that need them.
            </p>
            <p class="resumed">
              The imbalance of examples can create biases in the neural network, favoring categories with more examples.
              To avoid this, it is important to balance the dataset. Since some categories have fewer than 400 examples,
              we will use the available "other_images" to fill in the necessary images.
            </p>
            <h5>Satisfactory Examples</h5>
            <p>After a not-so-brief inspection, we observed satisfactory cases where the "other_images" are sufficiently
              similar (but not identical) to the original product.</p>


            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Object from the <i
                  class="variable">SOFA</i> category</h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <img src="images/ABO/SOFA_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/SOFA_OTH_3.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/SOFA_OTH_1.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/SOFA_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <h5>Problematic Examples</h5>
            <p>However, we also found images that are not of the product itself, but rather of a descriptive table, a
              color, or a general shot where the object is almost indistinguishable.</p>

            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Object from the <i
                  class="variable">LEGUME</i> category</h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image" style="max-width: 170px;">
                  <span>
                    <img src="images/ABO/LEGUME_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/LEGUME_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_4.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>

            <div class="gtr-uniform home-images extended"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Object from the <i
                  class="variable">RUG</i> category
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <img src="images/ABO/RUG_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/RUG_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/RUG_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/RUG_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <p class="extended">This is an issue for us. We are interested in having some variability in the images
              to make our network more robust, but when we have overly complex images or images that don’t even contain
              the object itself, they harm the network's training, as it will associate incorrect patterns with the
              category in question. Remember the <i class="variable">RUG</i> example! It will be a problem in the
              future.
            </p>
            <p class="extended">To overcome this issue, we will filter the "other_images" using pre-trained neural
              networks. In this case, we will use the VGG16 model, removing the classification layers. This will leave
              us with a network that only detects features in an image, but doesn’t classify it.

              With this network, we will proceed to extract the features from the main image of each object (main_image)
              and then compare those features with each of the object's "other_images," obtaining a similarity
              coefficient between them. This coefficient will indicate how similar the "other_images" are to the main
              image, assigning a very low value to those that are not similar.
            </p>

            <p class="resumed">
              These cases complicate training, as overly complex or irrelevant images cause the network to learn
              incorrect patterns. To solve this, we will use the VGG16 model, removing the classification layers, to
              extract and compare the features between the main image and the "other_images." This way, we will obtain a
              similarity coefficient that will help us filter out less useful images.
            </p>
            <p style="margin-bottom: 10px;">Here are some examples of the use of this technique:</p>

            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores of a <i
                  class="variable">SOFA</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>

                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/SOFA_MAIN.jpg" alt="" />
                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
              display: flex;
              flex-direction: column;
              height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                    
                      margin: 0px;
                      margin-bottom: -9px;
                      font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.861</i></p>
                      <progress value="0.8615963" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.640</i></p>
                      <progress value="0.6404396" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.599</i></p>
                      <progress value="0.5996146" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.196</i></p>
                      <progress value="0.1969997" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_4.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores of an <i
                  class="variable">OTTOMAN</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>

                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/OTTOMAN-MAIN.jpg" alt="" />
                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
              display: flex;
              flex-direction: column;
              height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                    
                      margin: 0px;
                      margin-bottom: -9px;
                      font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.819</i></p>
                      <progress value="0.819" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.8198348.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.689</i></p>
                      <progress value="0.689" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.6890952.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.581</i></p>
                      <progress value="0.581" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.5811923.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.192</i></p>
                      <progress value="0.19274572" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.19274572.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>

            <p class="extended" style="margin-bottom: 10px;">Great! We see that it works; however, we encountered some
              issues:</p>
            <p class="extended">The RUG category gives us some trouble. If we look at a few examples, we can see that
              the "main_image" often shows the rug in a generic scene, somewhat "hidden." This causes the similarity
              coefficient of the "other_images" to be very low, leaving out many useful images.</p>

            <div class="gtr-uniform home-images extended"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores of a <i
                  class="variable">RUG</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/RUG-MAIN-SIM.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.273</i></p>
                      <progress value="0.27311817" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.27311817.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.213</i></p>
                      <progress value="0.21372926" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.21372926.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.187</i></p>
                      <progress value="0.18717194" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.18717194.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <p class="extended">We will use a threshold of 0.5 as the criterion to select the "other_images," choosing
              those with a similarity coefficient higher than this to reach 400 examples by category.
              However, for the <i class="variable">RUG</i> category, we will use a threshold of only 0.2 because, due to
              the characteristics of the "main_images," most of the "other_images" would be excluded.</p>
            <p class="resumed">Great! We see that it works, so we will use a threshold of 0.5 as the criterion to select
              the "other_images," choosing those with a similarity coefficient higher than this to complete the missing
              images in a category.</p>

            <h3 class="orange_title major">Final Dataset</h3>
            <p class="resumed">
              To wrap up, we organized the images by category using the similarity scores of the "other_images" to fill
              in those with few examples. The resulting dataset has 65 categories and about 25,800 images. Although some
              categories didn't reach 400 images, those with more than 320 were considered sufficient.
            </p>
            <p class="extended">
              Finally, to complete the dataset and train the neural network, we need to organize the images, grouping
              them by category. Once we know the similarity scores of the "other_images," we move all the "main_images"
              to their respective category folders and fill in those with less than 400 examples with the "other_images"
              of
              highest similarity score.
            </p>
            <p style="margin-bottom: 10px;">
              Here is the balance of the resulting dataset, much better!
            </p>

            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS FINAL.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Final dataset</i></p>
              </span>
            </div>
            <p id="categorias" class="extended">
              This dataset contains 65 categories and approximately 25,800 images. We acknowledge that some categories
              did not reach 400 images due to the lack of a good similarity score, but since they still have a
              considerable amount (>320), we will simply overlook it.
            </p>
            <h5 style="margin-bottom: 10px;" class="extended">
              Final Categories:
            </h5>

            <div class="categories-container extended"
              style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 40px;">
              <!-- Categorías comunes diarias -->
              <div class="category-group" style="width: 100%;">
                <i class="variable_2">COFFEE</i>
                <i class="variable_2">TEA</i>
                <i class="variable_2">BREAD</i>
                <i class="variable_2">DRINKING_CUP</i>
                <i class="variable_2">HEADPHONES</i>
                <i class="variable_2">CHARGING_ADAPTER</i>
                <i class="variable_2">SHOES</i>
                <i class="variable_2">PILLOW</i>
                <i class="variable_2">CHAIR</i>
                <i class="variable_2">WALL_ART</i>
                <i class="variable_2">LAMP</i>
                <i class="variable_2">RING</i>
                <i class="variable_2">HAT</i>
                <i class="variable_2">BACKPACK</i>
                <i class="variable_2">SUITCASE</i>
                <i class="variable_2">PLANTER</i>
                <i class="variable_2">WALLET</i>
              </div>

              <button id="show-more-btn" onclick="showMoreCategories()" style="margin-top: -20px;">...see more</button>

              <!-- Comida y Salud -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Health and food</h5>
                <i class="variable_2">LEGUME</i>
                <i class="variable_2">HERB</i>
                <i class="variable_2">HEALTH_PERSONAL_CARE</i>
                <i class="variable_2">SKIN_CLEANING_AGENT</i>
                <i class="variable_2">SKIN_MOISTURIZER</i>
                <i class="variable_2">BEAUTY</i>
                <i class="variable_2">VITAMIN</i>
                <i class="variable_2">NUTRITIONAL_SUPPLEMENT</i>
              </div>

              <!-- Muebles -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Furniture</h5>
                <i class="variable_2">SHELF</i>
                <i class="variable_2">CABINET</i>
                <i class="variable_2">DESK</i>
                <i class="variable_2">TABLE</i>
                <i class="variable_2">HEADBOARD</i>
                <i class="variable_2">BED</i>
                <i class="variable_2">OTTOMAN</i>
                <i class="variable_2">STOOL_SEATING</i>
                <i class="variable_2">SOFA</i>
              </div>

              <!-- Decoración y Ropa de Cama -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Deco and bed</h5>
                <i class="variable_2">RUG</i>
                <i class="variable_2">FLAT_SHEET</i>
                <i class="variable_2">FURNITURE_COVER</i>
                <i class="variable_2">LIGHT_FIXTURE</i>
              </div>

              <!-- Accesorios y Joyas -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Accessories and jewelry</h5>
                <i class="variable_2">NECKLACE</i>
                <i class="variable_2">EARRING</i>
                <i class="variable_2">ACCESSORY</i>
                <i class="variable_2">HANDBAG</i>
                <i class="variable_2">BOOT</i>
                <i class="variable_2">SANDAL</i>
                <i class="variable_2">PORTABLE_ELECTRONIC_DEVICE_COVER</i>
                <i class="variable_2">CELLULAR_PHONE_CASE</i>
                <i class="variable_2">SCREEN_PROTECTOR</i>
              </div>

              <!-- Artículos de Oficina y Limpieza -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Office products and cleaning</h5>
                <i class="variable_2">OFFICE_PRODUCTS</i>
                <i class="variable_2">STORAGE_BINDER</i>
                <i class="variable_2">STORAGE_HOOK</i>
                <i class="variable_2">CLEANING_AGENT</i>
                <i class="variable_2">BATTERY</i>
              </div>

              <!-- Otros -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Other</h5>
                <i class="variable_2">AUTO_ACCESSORY</i>
                <i class="variable_2">TOOLS</i>
                <i class="variable_2">SAFETY_SUPPLY</i>
                <i class="variable_2">FOOD_SERVICE_SUPPLY</i>
                <i class="variable_2">BISS</i>
                <i class="variable_2">LIGHT_BULB</i>
                <i class="variable_2">OUTDOOR_LIVING</i>
                <i class="variable_2">PET_SUPPLIES</i>
              </div>
            </div>
          </section>
        </div>
      </div>

      <div class="wrapper style3">
        <div class="inner">
          <section>
            <h2 class="white-big major">Neural Network Design and Training</h2>
            <h3 class="orange_title major">Network Training</h3>
            <div class="extended">
              <p>Achieving good network performance was challenging. As previously mentioned, we opted for transfer
                learning, starting with a pre-trained model without its top layers (the classification layers). This
                model would be responsible for detecting the main features in the images, and then custom layers would
                be added to classify these features into the 65 possible categories. Finally, fine-tuning was applied to
                optimize performance.</p>

              <p>Initially, we used the VGG-16 model, adding several classification layers (3 dense layers and 1 dropout
                layer). This model performed very poorly.</p>

              <p>We decided to simplify the problem by reducing the number of categories and also using Inception-v3.
                Improvements started to show, especially when the classification stage was simplified to 2 dense layers,
                1 dropout, and 1 BatchNormalization layer.</p>

              <p>After several iterations, satisfactory metrics were achieved. The most performant model only adds a
                single dense layer with 256 units, accompanied by a Dropout(0.4) and a data augmentation layer with
                various techniques to avoid overfitting. Surprisingly, this simple network is one of the most
                performant. This suggests that the Inception-v3 model already does an excellent job at detecting the
                features in an image, leaving little work to complete the model.</p>

              <p>Once we found an efficient network design, we continued testing, studying what benefits and harms the
                model. In the following table, you can see the metrics of the different designs tested:</p>
            </div>
            <div class="resumed">
              <p>Achieving good performance was challenging. Transfer learning was used, starting with a pre-trained
                model without its classification layers, and custom layers were added to classify the 65 categories.
                Initially, VGG-16 and several dense layers were tested, but performance was poor.</p>

              <p>By simplifying the problem by reducing categories and using Inception-v3 with a simpler architecture (2
                dense layers, dropout, and BatchNormalization), significant improvements were observed. Finally, the
                most efficient model only added a 256-unit dense layer, dropout, and data augmentation, demonstrating
                that Inception-v3 already detects features very well, leaving little additional work.</p>

              <p>Below are the metrics for the different designs tested:</p>
            </div>

            <div class="gtr-uniform home-images"
              style="margin-bottom: 10px; flex-direction: column; align-items: flex-start; background-color: #313345">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Comparison table of the models</h5>
              <table id="score-table" style="margin: 0px">
                <thead>
                  <tr>
                    <th>Version</th>
                    <th style="position: relative">Dense Layers <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Fully connected layers where each neuron is connected to all neurons
                          in the previous layer.</span>
                      </span></th>
                    <th style="position: relative">Data <br>Augmentation <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Technique to increase the dataset size by applying transformations
                          such as rotations, cropping, or image flipping.</span>
                      </span></th>
                    <th style="position: relative; padding-right: 29px;">BatchNorm <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Batch normalization, which speeds up training and improves network
                          stability.</span>
                      </span></th>
                    <th style="position: relative; padding-right: 29px;">Dropout <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Technique to prevent overfitting by randomly dropping neurons during
                          training.</span>
                      </span></th>
                    <th class="separator"></th>
                    <th style="position: relative; padding-right: 29px;">Accuracy <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Model accuracy, indicating the proportion of correct
                          predictions.</span>
                      </span></th>
                    <th style="position: relative">Loss <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Measures how different the model's predictions are compared to the
                          actual values.</span>
                      </span></th>
                    <th style="position: relative">Val Accuracy <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Model accuracy on the validation set, indicating performance outside
                          the training set.</span>
                      </span></th>
                    <th style="position: relative">Val Loss <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Measures how different the model's predictions are on the validation
                          set to detect overfitting.</span>
                      </span></th>


                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="description">v1.1</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.89</td>
                    <td class="loss">0.35</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.48</td>
                  </tr>
                  <tr>
                    <td class="description">v1.2</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">❌</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.97</td>
                    <td class="loss">0.09</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.56</td>
                  </tr>
                  <tr>
                    <td class="description">v1.3</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">❌</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.91</td>
                    <td class="loss">0.26</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.46</td>
                  </tr>
                  <tr>
                    <td class="description">v1.4</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">❌</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.95</td>
                    <td class="loss">0.15</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.51</td>
                  </tr>
                  <tr>
                    <td class="description">v2.1</td>
                    <td>1x(256) 1x(512)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.83</td>
                    <td class="loss">0.57</td>
                    <td class="val_accuracy">0.84</td>
                    <td class="val_loss">0.54</td>
                  </tr>
                  <tr>
                    <td class="description">v2.2</td>
                    <td>1x(512) 1x(1024)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.85</td>
                    <td class="loss">0.47</td>
                    <td class="val_accuracy">0.84</td>
                    <td class="val_loss">0.55</td>
                  </tr>
                  <!-- Add more rows as needed -->
                </tbody>
              </table>
              <p style="margin-bottom: 0px; margin-top: 0px;"><i style="font-size: 14px">All models use Inception-v3 and
                  were trained with 20 epochs in the initial training and 15 in the fine-tuning stage</i></p>

              </span>
            </div>
            <div class="extended">
              <p>Analyzing these statistics, we can observe substantial improvements in accuracy and loss values in
                models v1.2 and v1.4, but at the same time, we notice slightly worse val_loss values, indicating
                overfitting, which makes sense. The data augmentation layer aims to make our model more robust by
                altering images in various ways, such as random rotations, changes in contrast or brightness, random
                zooms, etc. Additionally, the primary goal of dropout layers is to prevent overfitting, so it is
                understandable that performance worsens in some cases.</p>

              <p>On the other hand, we see that model v1.3, which lacks the BatchNormalization layer, shows an
                interesting improvement in performance. While these types of layers are very important and often used in
                image classification models, we can attribute this drop in performance to the fact that it is being used
                in the final classification stage of the model. It might be more useful in an intermediate stage of a
                more complex model.</p>

              <p>Another interesting observation from the statistics is the similarity in performance between models
                regarding val_accuracy. As we can see, all models have very similar values. My theory is that this is
                due to several validation images simply being mislabeled. These are similar cases to the RUG category,
                where the first image not only contains the rug but also includes other objects like sofas, chairs,
                paintings, etc.

                Considering this, we can assume that the model will never be able to surpass a certain level of
                performance because some images are classified under a specific category but contain objects from
                another. This is an area worth studying and improving.</p>

              <p>Finally, we observe that increasing the complexity of the model only worsens the performance, which is
                somewhat surprising, but on the other hand, it makes sense since the Inception_v3 base model is very
                good at its job, and possibly its output cannot be improved. This leaves us with the sole task of
                classifying the already well defined and detected features into the x categories of our problem.</p>

            </div>

            <div class="resumed">
              <p>When analyzing the statistics, improvements in accuracy and loss are observed in models v1.2 and v1.4,
                but an increase in val_loss suggests overfitting, which is understandable due to the underuse of data
                augmentation and dropouts to prevent it. In model v1.3, the absence of BatchNormalization improved
                performance, possibly because this layer is more useful in intermediate stages of complex models.</p>
              <p>
                Additionally, the similarities in val_accuracy across models could be due to mislabeled images in the
                validation set, limiting the maximum achievable performance. Finally, increasing the complexity of the
                model worsens its performance, reinforcing the idea that Inception_v3 already does an excellent job at
                detecting features, leaving little room for improvement.</p>
            </div>

            <h3 class="orange_title major">Selected Model Statistics</h3>
            <p style="margin-bottom: 20px">
              The winning model was v1.3. You can check out the complete code, explained in detail, in this <a
                class="a-custom" target="_blank"
                href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj?usp=sharing">Colab</a>;
              I recommend giving it a look. Below are some statistics of the model:
            </p>

            <div class="gtr-uniform home-images" style="margin-bottom: 25px; background-color: #313345">
              <span class=" image fit" style="margin: 0px;">
                <h5 style="margin: 0px; margin-left: 7px; margin-bottom: 15px;">Initial training</h5>
                <img src="images/ABO/train v1.3.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 19px;">
                  Unlike the other models, the training for this one was increased from 20 epochs to 40 to squeeze out a
                  bit more performance. The training took approximately 13 minutes.
                </p>

              </span>
            </div>
            <div class="gtr-uniform home-images" style="margin-bottom: 50px; background-color: #313345">
              <span class=" image fit" style="margin: 0px;">
                <h5 style="margin: 0px; margin-left: 7px; margin-bottom: 15px;">Fine tunning</h5>
                <img src="images/ABO/fine v1.3.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 19px;">
                  Similarly, the epochs in the fine-tuning stage were increased to 25, although, as seen in the graph,
                  signs of overfitting start to appear around epoch 10-15, with little improvement in val_accuracy and
                  val_loss, which are the metrics we are most interested in. The fine-tuning took approximately 30
                  minutes.
                </p>

              </span>
            </div>
            <div class="extended">
              <h5>Category Statistics</h5>
              <p>In the following table, you can see the Precision, Recall, and F1 values for each category, where the
                first ones are those with the worst performance.</p>

              <table id="score-table_2">
                <thead>
                  <tr>
                    <th>Category
                    </th>
                    <th style="position: relative">Precision
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Precision measures how many of the positive predictions are
                          correct.</span>
                      </span>
                    </th>
                    <th style="position: relative">Recall
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Recall measures how many of the actual positives were correctly
                          identified.</span>
                      </span>
                    </th>
                    <th style="position: relative">F1-Score
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">F1 is the harmonic mean between precision and recall.</span>
                      </span>
                    </th>
                    <th style="position: relative">Support
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Support is the total number of occurrences of a specific class in the
                          dataset.</span>
                      </span>
                    </th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>HEALTH_PERSONAL_CARE</td>
                    <td>0.71</td>
                    <td>0.57</td>
                    <td>0.63</td>
                    <td>83</td>
                  </tr>
                  <tr>
                    <td>SHOES</td>
                    <td>0.80</td>
                    <td>0.68</td>
                    <td>0.74</td>
                    <td>66</td>
                  </tr>
                  <tr>
                    <td>LUGGAGE</td>
                    <td>0.80</td>
                    <td>0.67</td>
                    <td>0.73</td>
                    <td>83</td>
                  </tr>
                  <tr>
                    <td>ACCESSORY</td>
                    <td>0.85</td>
                    <td>0.70</td>
                    <td>0.77</td>
                    <td>76</td>
                  </tr>
                  <tr>
                    <td>CHAIR</td>
                    <td>0.82</td>
                    <td>0.70</td>
                    <td>0.76</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OUTDOOR_LIVING</td>
                    <td>0.81</td>
                    <td>0.69</td>
                    <td>0.75</td>
                    <td>81</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BISS</td>
                    <td>0.69</td>
                    <td>0.63</td>
                    <td>0.66</td>
                    <td>73</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BEAUTY</td>
                    <td>0.68</td>
                    <td>0.65</td>
                    <td>0.67</td>
                    <td>75</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OFFICE_PRODUCTS</td>
                    <td>0.67</td>
                    <td>0.65</td>
                    <td>0.66</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>WASTE_BAG</td>
                    <td>0.86</td>
                    <td>0.90</td>
                    <td>0.88</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>NUTRITIONAL_SUPPLEMENT</td>
                    <td>0.71</td>
                    <td>0.82</td>
                    <td>0.76</td>
                    <td>74</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SKIN_CLEANING_AGENT</td>
                    <td>0.78</td>
                    <td>0.82</td>
                    <td>0.80</td>
                    <td>93</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>AUTO_ACCESSORY</td>
                    <td>0.69</td>
                    <td>0.85</td>
                    <td>0.76</td>
                    <td>65</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>VITAMIN</td>
                    <td>0.82</td>
                    <td>0.75</td>
                    <td>0.79</td>
                    <td>85</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HOME_BED_AND_BATH</td>
                    <td>0.86</td>
                    <td>0.74</td>
                    <td>0.80</td>
                    <td>74</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SHELF</td>
                    <td>0.89</td>
                    <td>0.76</td>
                    <td>0.82</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FOOD_SERVICE_SUPPLY</td>
                    <td>0.75</td>
                    <td>0.82</td>
                    <td>0.79</td>
                    <td>78</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TOOLS</td>
                    <td>0.84</td>
                    <td>0.78</td>
                    <td>0.81</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>COFFEE</td>
                    <td>0.85</td>
                    <td>0.91</td>
                    <td>0.88</td>
                    <td>76</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>WALLET</td>
                    <td>0.83</td>
                    <td>0.93</td>
                    <td>0.88</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>NECKLACE</td>
                    <td>0.99</td>
                    <td>1.00</td>
                    <td>0.99</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BATTERY</td>
                    <td>0.96</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HANDBAG</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>75</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BACKPACK</td>
                    <td>0.82</td>
                    <td>0.97</td>
                    <td>0.89</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BREAD</td>
                    <td>0.97</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BED</td>
                    <td>0.86</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>73</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BOOT</td>
                    <td>0.89</td>
                    <td>0.95</td>
                    <td>0.92</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CABINET</td>
                    <td>0.90</td>
                    <td>0.91</td>
                    <td>0.90</td>
                    <td>95</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CHARGING_ADAPTER</td>
                    <td>0.93</td>
                    <td>0.91</td>
                    <td>0.92</td>
                    <td>81</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CLEANING_AGENT</td>
                    <td>0.89</td>
                    <td>0.87</td>
                    <td>0.88</td>
                    <td>82</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>DESK</td>
                    <td>0.91</td>
                    <td>0.90</td>
                    <td>0.90</td>
                    <td>79</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>DRINKING_CUP</td>
                    <td>0.92</td>
                    <td>0.95</td>
                    <td>0.94</td>
                    <td>76</td>
                  </tr>
                  <tr>
                    <td>EARRING</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FLAT_SHEET</td>
                    <td>0.91</td>
                    <td>0.91</td>
                    <td>0.91</td>
                    <td>66</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FURNITURE_COVER</td>
                    <td>0.88</td>
                    <td>0.93</td>
                    <td>0.90</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HARDWARE_HANDLE</td>
                    <td>0.87</td>
                    <td>0.95</td>
                    <td>0.91</td>
                    <td>62</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HAT</td>
                    <td>0.86</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HEADBOARD</td>
                    <td>0.94</td>
                    <td>0.93</td>
                    <td>0.94</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HEADPHONES</td>
                    <td>0.96</td>
                    <td>0.94</td>
                    <td>0.95</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HERB</td>
                    <td>0.99</td>
                    <td>0.94</td>
                    <td>0.96</td>
                    <td>71</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LAMP</td>
                    <td>0.89</td>
                    <td>0.94</td>
                    <td>0.92</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LEGUME</td>
                    <td>0.96</td>
                    <td>0.99</td>
                    <td>0.97</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LIGHT_BULB</td>
                    <td>0.91</td>
                    <td>0.97</td>
                    <td>0.94</td>
                    <td>62</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LIGHT_FIXTURE</td>
                    <td>0.90</td>
                    <td>0.89</td>
                    <td>0.90</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OTTOMAN</td>
                    <td>0.85</td>
                    <td>0.88</td>
                    <td>0.86</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PET_SUPPLIES</td>
                    <td>0.92</td>
                    <td>0.79</td>
                    <td>0.85</td>
                    <td>72</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PILLOW</td>
                    <td>0.87</td>
                    <td>0.98</td>
                    <td>0.92</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PLANTER</td>
                    <td>0.88</td>
                    <td>0.99</td>
                    <td>0.93</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PORTABLE_ELECTRONIC_DEVICE_COVER</td>
                    <td>0.96</td>
                    <td>0.87</td>
                    <td>0.91</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>RING</td>
                    <td>1.00</td>
                    <td>0.94</td>
                    <td>0.97</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>RUG</td>
                    <td>0.98</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>90</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SAFETY_SUPPLY</td>
                    <td>0.89</td>
                    <td>0.84</td>
                    <td>0.86</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SANDAL</td>
                    <td>0.87</td>
                    <td>0.93</td>
                    <td>0.90</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SAUTE_FRY_PAN</td>
                    <td>0.96</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SCREEN_PROTECTOR</td>
                    <td>0.96</td>
                    <td>0.99</td>
                    <td>0.97</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SKIN_MOISTURIZER</td>
                    <td>0.91</td>
                    <td>0.89</td>
                    <td>0.90</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SOFA</td>
                    <td>0.79</td>
                    <td>0.86</td>
                    <td>0.82</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STOOL_SEATING</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>79</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STORAGE_BINDER</td>
                    <td>0.93</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STORAGE_HOOK</td>
                    <td>0.94</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>90</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SUITCASE</td>
                    <td>0.87</td>
                    <td>0.95</td>
                    <td>0.91</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TABLE</td>
                    <td>0.82</td>
                    <td>0.79</td>
                    <td>0.80</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TEA</td>
                    <td>0.87</td>
                    <td>0.96</td>
                    <td>0.91</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Accuracy</strong></td>
                    <td colspan="4">0.88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Macro avg</strong></td>
                    <td>0.87</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>5167</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Weighted avg</strong></td>
                    <td>0.88</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>5167</td>
                  </tr>
                </tbody>
              </table>


              <button id="show-more-table-btn" onclick="showMoreStats()"
                style="margin-top: -20px; margin-bottom: 60px">...expand</button>
            </div>

            <h3 class="orange_title major">Model in Action</h3>
            <p style="margin-bottom: 20px">A space was created on HuggingFace Spaces to keep the model active using
              Gradio, allowing it to be used at any time. You can try it out below, test it uploading an image from
              internet, or even taking an photo with your device!</p>

            <div style="text-align: center;">
              <iframe allow="camera" src="https://nicolaspavon-amazon-classification.hf.space" frameborder="0"
                class="gradio"></iframe>
            </div>

            <p class="extended" style="margin-bottom: 20px; margin-top: 10px;">Here I list the most commonly accessible
              categories (in my opinion), but you can photograph any object from <a class="a-custom"
                href="#categorias">the 65 categories</a> to test the model.</p>
            <p class="resumed" style="margin-bottom: 20px; margin-top: 10px;">Here I list the most commonly accessible
              categories (in my opinion), but you can photograph any object from the 65 categories to test the model.
            </p>

            <div class="category-group-2 extended" style="width: 100%;">
              <i class="variable_2">COFFEE</i>
              <i class="variable_2">TEA</i>
              <i class="variable_2">BREAD</i>
              <i class="variable_2">DRINKING_CUP</i>
              <i class="variable_2">HEADPHONES</i>
              <i class="variable_2">CHARGING_ADAPTER</i>
              <i class="variable_2">SHOES</i>
              <i class="variable_2">PILLOW</i>
              <i class="variable_2">CHAIR</i>
              <i class="variable_2">WALL_ART</i>
              <i class="variable_2">LAMP</i>
              <i class="variable_2">RING</i>
              <i class="variable_2">HAT</i>
              <i class="variable_2">BACKPACK</i>
              <i class="variable_2">SUITCASE</i>
              <i class="variable_2">PLANTER</i>
              <i class="variable_2">WALLET</i>
            </div>
            <div class="resumed">
              <h5 style="margin-bottom: 10px;">
                Final categories:
              </h5>
              <div class="categories-container" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 40px;">
                <!-- Categorías comunes diarias -->
                <div class="category-group" style="width: 100%;">
                  <i class="variable_2">COFFEE</i>
                  <i class="variable_2">TEA</i>
                  <i class="variable_2">BREAD</i>
                  <i class="variable_2">DRINKING_CUP</i>
                  <i class="variable_2">HEADPHONES</i>
                  <i class="variable_2">CHARGING_ADAPTER</i>
                  <i class="variable_2">SHOES</i>
                  <i class="variable_2">PILLOW</i>
                  <i class="variable_2">CHAIR</i>
                  <i class="variable_2">WALL_ART</i>
                  <i class="variable_2">LAMP</i>
                  <i class="variable_2">RING</i>
                  <i class="variable_2">HAT</i>
                  <i class="variable_2">BACKPACK</i>
                  <i class="variable_2">SUITCASE</i>
                  <i class="variable_2">PLANTER</i>
                  <i class="variable_2">WALLET</i>
                </div>

                <button id="show-more-btn-2" onclick="showMoreCategories()" style="margin-top: -20px;">...see more
                </button>

                <!-- Comida y Salud -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Food and health</h5>
                  <i class="variable_2">LEGUME</i>
                  <i class="variable_2">HERB</i>
                  <i class="variable_2">HEALTH_PERSONAL_CARE</i>
                  <i class="variable_2">SKIN_CLEANING_AGENT</i>
                  <i class="variable_2">SKIN_MOISTURIZER</i>
                  <i class="variable_2">BEAUTY</i>
                  <i class="variable_2">VITAMIN</i>
                  <i class="variable_2">NUTRITIONAL_SUPPLEMENT</i>
                </div>

                <!-- Muebles -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Furniture</h5>
                  <i class="variable_2">SHELF</i>
                  <i class="variable_2">CABINET</i>
                  <i class="variable_2">DESK</i>
                  <i class="variable_2">TABLE</i>
                  <i class="variable_2">HEADBOARD</i>
                  <i class="variable_2">BED</i>
                  <i class="variable_2">OTTOMAN</i>
                  <i class="variable_2">STOOL_SEATING</i>
                  <i class="variable_2">SOFA</i>
                </div>

                <!-- Decoración y Ropa de Cama -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Deco and bed</h5>
                  <i class="variable_2">RUG</i>
                  <i class="variable_2">FLAT_SHEET</i>
                  <i class="variable_2">FURNITURE_COVER</i>
                  <i class="variable_2">LIGHT_FIXTURE</i>
                </div>

                <!-- Accesorios y Joyas -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Accessories and jewelry</h5>
                  <i class="variable_2">NECKLACE</i>
                  <i class="variable_2">EARRING</i>
                  <i class="variable_2">ACCESSORY</i>
                  <i class="variable_2">HANDBAG</i>
                  <i class="variable_2">BOOT</i>
                  <i class="variable_2">SANDAL</i>
                  <i class="variable_2">PORTABLE_ELECTRONIC_DEVICE_COVER</i>
                  <i class="variable_2">CELLULAR_PHONE_CASE</i>
                  <i class="variable_2">SCREEN_PROTECTOR</i>
                </div>

                <!-- Artículos de Oficina y Limpieza -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Office and cleaning products </h5>
                  <i class="variable_2">OFFICE_PRODUCTS</i>
                  <i class="variable_2">STORAGE_BINDER</i>
                  <i class="variable_2">STORAGE_HOOK</i>
                  <i class="variable_2">CLEANING_AGENT</i>
                  <i class="variable_2">BATTERY</i>
                </div>

                <!-- Otros -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Other</h5>
                  <i class="variable_2">AUTO_ACCESSORY</i>
                  <i class="variable_2">TOOLS</i>
                  <i class="variable_2">SAFETY_SUPPLY</i>
                  <i class="variable_2">FOOD_SERVICE_SUPPLY</i>
                  <i class="variable_2">BISS</i>
                  <i class="variable_2">LIGHT_BULB</i>
                  <i class="variable_2">OUTDOOR_LIVING</i>
                  <i class="variable_2">PET_SUPPLIES</i>
                </div>
              </div>
            </div>

            <p>
              In <a class="a-custom"
                href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj#scrollTo=WKmy4s-RAc1K&line=4&uniqifier=1">the
                Colab</a>, you can see the tests that were conducted to validate the model. I used images already
              seen by the model during training, and also new images manually extracted from Amazon.
            </p>
            <p>
              In these tests, the previously mentioned issues can be observed. The categories <i
                class="variable">SUITCASE</i> and <i class="variable">LUGGAGE</i> are often confused, and the category
              <i class="variable">RUG</i> frequently appears in photos of sofas or tables, among other issues.
            </p>

          </section>
        </div>
      </div>
      <div class="wrapper alt style4">
        <div class="inner">
          <section>
            <h3 class="orange_title major">Things to improve</h3>
            <div>
              <p>As observed during the dataset analysis phase, a large number of <i class="variable">RUG</i> images
                contain other elements like sofas, chairs, tables, among others. This hinders the network, causing
                images of sofas, for example, to be incorrectly classified under the <i class="variable">RUG</i>
                category. Removing these "generic" images from the dataset would significantly improve performance.</p>

              <p>The dataset also contains many images that show the texture or color of the item for sale, which does
                not contribute to the network's training and are probably affecting the loss and val_loss statistics.
              </p>

              <p>The <i class="variable">FURNITURE_COVER</i> category is very similar to categories like <i
                  class="variable">SOFA</i> or <i class="variable">CHAIR</i>, leading to incorrect classifications.</p>

              <p>As mentioned in the <a class="a-custom" href="#inspeccion">dataset inspection</a> stage, there are
                several categories that are very generic or easily confused. As expected, when testing the model with
                objects from <i class="variable">SUITCASE</i> and <i class="variable">LUGGAGE</i>, it tends to
                misclassify them. The same happens with <i class="variable">NUTRITIONAL_SUPPLEMENT</i> ↔ <i
                  class="variable">VITAMINS</i> ↔ <i class="variable">HEALTH_PERSONAL_CARE</i>, which are often images
                of bottles. In the <a class="a-custom" href="#score-table_2">model statistics</a>, you can see the poor
                performance of these categories due to their similarity.</p>

              <p>Removing or merging some of these categories would solve most of the model's issues.</p>
            </div>

            <div class="extended">
              <h3 class="orange_title major">Things that didn't work</h3>
              <h5>Clustering of Macro-Categories</h5>
              <p>Initially, the model's poor performance was attributed to the large number of categories. For this
                reason, the possibility of creating a generic model to classify within 4-5 macro-categories was
                considered, followed by applying another sub-model for each macro-category to identify the final
                category. To do this, a clustering technique similar to the one used in dataset balancing was employed,
                grouping similar categories and then generating macro-categories from these groups. Below is the
                generated graph for the grouping:</p>
              <div class="gtr-uniform home-images" style="margin-bottom: 10px">
                <span class="image fit" style="margin: 0px;"><img src="images/ABO/class clustering.png" alt="" />
                  <p style="margin-bottom: 0px; margin-top: 10px;"><i>Category groupings by similarity</i></p>
                </span>
              </div>
              <p>As seen, it successfully grouped categories like <i class="variable">SOFA</i>, <i
                  class="variable">RUG</i>, <i class="variable">CHAIR</i>, and others into a possible macro-category <i
                  class="variable">FURNITURE</i> (green branch). The same happened with other similar categories.</p>
              <p>Although this idea seemed promising, it was observed that the model was able to work successfully with
                all 65 categories, rendering this idea obsolete.</p>

              <h5>Black and White Images</h5>
              <p>We attempted to convert the images to black and white, as colors shouldn’t make a difference between
                categories. However, no improvement was noticed, although it may have been due to an implementation
                error.</p>

            </div>
          </section>
        </div>
      </div>
      <div id="conclusions" class="wrapper style2">
        <div class="inner">
          <section>
            <h2 class="white-big major">Conclusions</h2>
            <p>In this project, we observed the efficiency of transfer learning and fine-tuning techniques, and
              highlighted the importance of a thorough dataset analysis, achieving an efficient model despite the
              limitations encountered. Although there are areas for improvement, the results demonstrate the value of an
              incremental approach and constant experimentation to optimize deep learning models.</p>
            <p>
              Special thanks to Juan Kurucz for his guidance, and to my project teammates for their help at the
              beginning of the project: Tomás Rama, Matías Cabrera, Mauricio Gómez, and Agustín Lorenzo.
            </p>
            <p>
              That's all, thank you for your time! If you have any questions or comments, feel free to let me know below
              🙏
            </p>
          </section>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <section id="footer"></section>
  </div>

  <!-- Scripts -->
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/jquery.scrollex.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>
  <script src="scripts/menuEng.js"></script>
  <script src="scripts/footerEng.js"></script>

  <script>
    function applyColorToCell() {
      const table = document.getElementById('score-table');
      const rows = table.getElementsByTagName('tr');

      for (let i = 1; i < rows.length; i++) {
        const cells = rows[i].getElementsByTagName('td');

        const accuracy = parseFloat(cells[6].innerText);
        const loss = parseFloat(cells[7].innerText);
        const valAccuracy = parseFloat(cells[8].innerText);
        const valLoss = parseFloat(cells[9].innerText);

        // For accuracy and val_accuracy (higher is better)
        cells[6].style.backgroundColor = getGreenToRedGradient(accuracy, true);
        cells[8].style.backgroundColor = getGreenToRedGradient(valAccuracy, true);

        // For loss and val_loss (lower is better)
        cells[7].style.backgroundColor = getGreenToRedGradient(loss, false);
        cells[9].style.backgroundColor = getGreenToRedGradient(valLoss, false);
      }
    }

    function applyColorToCell_2() {
      const table = document.getElementById('score-table_2');
      const rows = table.getElementsByTagName('tr');
      console.log(rows.length)


      for (let i = 1; i < rows.length; i++) {
        const cells = rows[i].getElementsByTagName('td');
        console.log(cells)

        const precision = parseFloat(cells[1].innerText);
        const recall = parseFloat(cells[2].innerText);
        const f1 = parseFloat(cells[3].innerText);

        // For accuracy and val_accuracy (higher is better)
        cells[1].style.backgroundColor = getGreenToRedGradient(precision, true);
        cells[2].style.backgroundColor = getGreenToRedGradient(recall, true);
        cells[3].style.backgroundColor = getGreenToRedGradient(f1, true);
      }
    }

    function getGreenToRedGradient(value, higherIsBetter) {
      const colors = [
        { r: 0, g: 128, b: 0 },     // Green
        { r: 85, g: 170, b: 0 },    // Yellowgreen
        { r: 255, g: 255, b: 0 },   // Yellow
        { r: 255, g: 165, b: 0 },   // Orange
        { r: 255, g: 0, b: 0 },     // Red
        { r: 0, g: 0, b: 0 }        // Black
      ];

      // Adjust ratio for whether higher or lower is better
      let ratio = !higherIsBetter ? value : 1 - value;
      const step = 1 / (colors.length - 1);
      const index = Math.min(Math.floor(ratio / step), colors.length - 2);
      const color1 = colors[index];
      const color2 = colors[index + 1];

      const t = (ratio - index * step) / step;
      const r = Math.round(color1.r * (1 - t) + color2.r * t);
      const g = Math.round(color1.g * (1 - t) + color2.g * t);
      const b = Math.round(color1.b * (1 - t) + color2.b * t);

      return `rgb(${r}, ${g}, ${b}, 0.8)`; // Added opacity (0.8) to the colors
    }



    // Call the function to apply colors on page load
    applyColorToCell();
    applyColorToCell_2();
  </script>

</body>

</html>