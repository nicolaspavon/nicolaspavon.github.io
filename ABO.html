<!DOCTYPE html>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
  <title>ABO Image classification</title>
  <meta charset="utf-8" />
  <meta http-equiv="Permissions-Policy" content="camera=(self)">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" charset="UTF-8" />
  <link rel="stylesheet" href="assets/css/main.css" />
  <noscript>
    <link rel="stylesheet" href="assets/css/noscript.css" />
  </noscript>
  <link rel="icon" href="images/icon.webp" type="image/x-icon" />
  <script>
    function showMoreCategories() {
      const groups = document.querySelectorAll('.category-group');
      groups.forEach((group, index) => {
        if (index !== 0) {
          group.style.display = 'block';
        }
      });
      document.getElementById('show-more-btn').style.display = 'none';
      document.getElementById('show-more-btn-2').style.display = 'none';
    }
  </script>
  <script>
    function toggleVersion() {
      const resumed = document.querySelectorAll('.resumed');
      const extended = document.querySelectorAll('.extended');

      resumed.forEach((section) => {
        if (section.style.display === 'none' || window.getComputedStyle(section).display === 'none') {
          section.style.display = 'block';
          document.getElementById('complete-btn').style.display = 'none';
          document.getElementById('resumed-btn').style.display = 'flex';
        } else {
          section.style.display = 'none';
        }
      });

      extended.forEach((section) => {
        if (section.style.display === 'none' || window.getComputedStyle(section).display === 'none') {
          section.style.display = 'block';
          document.getElementById('resumed-btn').style.display = 'none';
          document.getElementById('complete-btn').style.display = 'flex';
        } else {
          section.style.display = 'none';
        }
      });
    }
  </script>

  <script>
    function showMoreStats() {
      const groups = document.querySelectorAll('.cat_stats');
      groups.forEach((group, index) => {
        if (index !== 0) {
          group.style.display = 'table-row';
        }
      });
      document.getElementById('show-more-table-btn').style.display = 'none';
    }
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      const progressBars = document.querySelectorAll(".progress-bar");

      // IntersectionObserver callback function
      const animateProgressBar = (entries, observer) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const progressBar = entry.target;
            const value = parseFloat(progressBar.getAttribute("value"));
            progressBar.setAttribute("value", "0"); // Start from 0

            // Animate the progress bar value
            let currentValue = 0;
            const increment = value / 100; // Adjust this for speed
            const interval = setInterval(() => {
              currentValue += increment;
              if (currentValue >= value) {
                currentValue = value;
                clearInterval(interval);
              }
              progressBar.setAttribute("value", currentValue);
            }, 15); // Adjust this for smoothness

            // Stop observing the progress bar after animation is done
            observer.unobserve(progressBar);
          }
        });
      };

      // Create an IntersectionObserver
      const observer = new IntersectionObserver(animateProgressBar, {
        threshold: 0.1 // Trigger when 10% of the element is visible
      });

      // Observe each progress bar
      progressBars.forEach(bar => observer.observe(bar));
    });
  </script>

  <style>
    .resumed {
      display: none
    }

    html {
      scroll-behavior: smooth;
    }

    h3 {
      color: #ee5f0f;
    }

    h4 {
      font-size: 0.9em;
    }

    h5 {
      font-size: 0.8em;
    }

    .white-big {
      font-size: 1.3em;
      text-align: center;
      margin-bottom: 20px;
      border-bottom: none !important;
    }

    .orange_title {
      font-size: 1.1em;
    }

    .variable {
      /* font-family: "Courier New", Courier, monospace; */
      background-color: #212430;
      padding: 2px 4px;
      color: #ee5f0f;
      font-size: 12px;
      font-weight: bold;
    }

    .variable_2 {
      /* font-family: "Courier New", Courier, monospace; */
      background-color: #212430;
      padding: 2px 4px;
      color: #ee5f0f;
      font-size: 16px;
      font-weight: bold;
    }

    .main-image {
      margin-right: 20px;
      height: auto;
      width: 100px;
      /* Maintain aspect ratio */
      object-fit: cover;
      /* Adjust the margin value as needed */
    }

    .oth-image {
      margin-right: 20px;
      height: auto;
      width: 100px;
      /* Maintain aspect ratio */
      object-fit: cover;
      /* Adjust the margin value as needed */
    }

    .gtr-uniform {
      display: flex;
      align-items: center;
      gap: 10px;
      /* Optional: Adds a gap between all columns */
    }

    .image-fit {
      width: 100%;
      max-width: 300px;
      height: auto;
      object-fit: cover;
    }

    /* Form Container */
    #contact-form {
      background-color: #2e3141;
      padding: 40px;
      border-radius: 8px;
    }

    /* Form Fields */
    .fields {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
    }

    .field {
      width: 100%;
    }

    /* Half-width fields for larger screens */
    .field.half {
      width: calc(50% - 10px);
    }

    .field.half.second {
      width: calc(50% - 10px);
      padding-right: 20px;
      padding-left: 0px;
    }

    /* Input and Textarea Styling */
    input[type="text"],
    input[type="email"],
    textarea {
      width: 100%;
      padding: 12px 15px;
      border: 1px solid #ccc;
      border-radius: 4px;
      background-color: #fff;
      color: #333;
      font-size: 16px;
    }

    /* Button Styling */
    .button {
      background-color: #ee5f0f;
      color: #fff;
      padding: 12px 30px;
      border: none;
      cursor: pointer;
      border-radius: 4px;
      font-size: 16px;
      text-align: center;
      display: block;
      margin: 0 auto;
      transition: background-color 0.3s;
      line-height: 0;
    }

    .button:hover {
      background-color: #ff7043;
    }

    /* Labels */
    label {
      color: #f0f0f0;
      font-size: 14px;
      margin-bottom: 8px;
      display: block;
    }

    /* Form Responsiveness */
    @media (max-width: 768px) {
      .field.half {
        width: 100%;
      }
    }

    /* Styling for the home images container */
    .home-images {
      display: flex;
      justify-content: space-between;
      background-color: #3a3d52;
      /* Background color for the row */
      padding: 20px;
      border-radius: 8px;
    }

    /* Styling for each image container */
    .home-image {
      flex: 1;
      margin: 0 10px;
      background-color: #4b4e63;
      /* Default background color for images */
      padding: 10px;
      border-radius: 8px;
      text-align: center;
    }

    /* Styling for the main image */
    .home-image.main-image {
      background-color: #5c5f75;
      max-width: 207px;
      /* Slightly different background color */
    }

    /* Styling for the images inside the containers */
    .home-image img {
      max-width: 100%;
      height: auto;
      border-radius: 4px;
    }

    .oth-row {
      display: flex;
      gap: 10px
    }

    .generic-cat {
      display: flex;
      flex-direction: row;
    }

    .gradio {
      width: 100%;
      height: auto;
      min-height: 483px;
    }

    /* Responsive adjustments */
    @media (max-width: 768px) {
      .home-images {
        flex-direction: column;
        gap: 20px;
      }

      .generic-cat {
        display: flex;
        flex-direction: column;
      }

      .home-image {
        margin: 10px 0;
      }

    }

    /* Responsive adjustments */
    @media (max-width: 878px) {

      .gradio {
        width: 100%;
        height: auto;
        min-height: 838px;
      }
    }

    /* Progress Bar Styling */
    .progress-bar {
      width: 100%;
      height: 8px;
      appearance: none;
      -webkit-appearance: none;
      margin-bottom: 5px;
    }

    /* Custom styles for different browsers */
    .progress-bar::-webkit-progress-bar {
      background-color: #f0f0f0;
      border-radius: 4px;
    }

    .progress-bar::-webkit-progress-value {
      background-color: #fe8d59;
      border-radius: 4px;
    }

    .progress-bar::-moz-progress-bar {
      background-color: #fe8d59;
      border-radius: 4px;
    }

    .category-group {
      margin-bottom: 20px;
    }

    .category-group-2 {
      margin-bottom: 20px;
    }

    .cat_stats {
      display: none;
    }

    #show-more-btn {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 11px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
    }

    #show-more-btn-2 {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 11px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
    }

    #show-more-table-btn {
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #complete-btn {
      display: flex;
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #resumed-btn {
      display: none;
      margin-top: 0px;
      padding: 10px;
      height: 25px;
      font-size: 13px;
      cursor: pointer;
      line-height: 5px;
      background-color: #ee5f0f;
      transition: background-color 0.3s;
      border-radius: 4px;
      margin-left: 3px;
      margin-bottom: 20px;
    }

    #show-more-btn:hover {
      background-color: #ff7043;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-family: Arial, sans-serif;
    }

    th,
    td {
      padding: 10px;
      text-align: center;
      border: 2px solid #666;
      font-weight: 600;
      /* Make text slightly bolder */
    }

    th {
      background-color: #2e3141;
      color: #ee5f0f;
      border-bottom: 3px solid #ee5f0f;
    }


    td.boolean-cell {
      font-size: 1.5em;
    }

    td.accuracy,
    td.loss,
    td.val_accuracy,
    td.val_loss {
      width: 70px;
      color: #fff;
      /* Ensure text contrast on colored background */
    }

    .green-check {
      color: #66bb6a;
    }

    .red-cross {
      color: #ff7043;
    }

    .separator {
      border: none;
      width: 1px;
    }

    td.separator {
      background-color: #2e3141;
      /* Set a consistent color for the separator column */
      border: none;
      /* Remove the border for the separator column */
    }

    tr:nth-child(odd) {
      background-color: #3a3b44;
      /* Softer color for odd rows */
    }

    tr:nth-child(even) {
      background-color: #404351;
      /* Slightly darker, but still subtle */
    }

    th,
    td {
      padding: 5px;
      /* Reduce padding to make rows less tall */
      text-align: center;
      border: 2px solid #666;
      font-weight: 600;
      /* Keep the bold text */
    }

    td.accuracy,
    td.loss,
    td.val_accuracy,
    td.val_loss {
      padding: 8px;
      /* Slightly larger padding for the score cells for readability */
    }

    tbody tr {
      height: 40px;
      /* Adjust this value to your preferred fixed row height */
    }

    td {
      vertical-align: middle;
      overflow: hidden;
      /* Prevent overflow from expanding the row */
      white-space: nowrap;
      /* Prevent text wrapping inside the cells */
      text-overflow: ellipsis;
      /* Add ellipsis if the text is too long */
    }

    table td {
      padding: 0em 0.35em;
    }

    table th {
      padding: 5px;
    }

    .a-custom {
      color: #ee5f0f;
      text-decoration: underline;
      cursor: pointer;
    }


    .tooltip-container {
      position: absolute;
      display: inline-block;
      cursor: pointer;
      right: 8px;
      top: 4px;
    }

    .tooltip-container .tooltiptext {
      visibility: hidden;
      width: 150px;
      background-color: #555;
      color: #fff;
      text-align: center;
      border-radius: 5px;
      padding: 5px;
      position: absolute;
      z-index: 1;
      bottom: 125%;
      /* Position above the icon */
      left: 50%;
      margin-left: -75px;
      opacity: 0;
      transition: opacity 0.3s;
      width: 200px;
    }

    .tooltip-container .tooltiptext::after {
      content: "";
      position: absolute;
      top: 100%;
      /* Arrow below the tooltip */
      left: 50%;
      margin-left: -5px;
      border-width: 5px;
      border-style: solid;
      border-color: #555 transparent transparent transparent;
    }

    .tooltip-container:hover .tooltiptext {
      visibility: visible;
      opacity: 1;
    }

    .info-icon {
      margin-left: 5px;
      color: #ee5f0f;
      font-weight: bold;
    }
  </style>
</head>

<body class="is-preload" style="
      background-image: linear-gradient(
          to top,
          rgba(46, 49, 65, 0.8),
          rgba(46, 49, 65, 0.8)
        ),
        url(images/ABO/cosas.webp);
    ">

  <!-- Disclaimer: El objetivo de este blog es demostrar mis habilidades en machine learning, porque mis habilidades como web developer no me enorgullecen 🤦‍♂️ -->

  <!-- Advertencia: Este código puede generar secuelas como falta de apetito, dolor de ojos, depresión y pérdida de cabello. Continúa leyendolo bajo tu propio riesgo -->

  <!-- Page Wrapper -->
  <div id="page-wrapper">
    <!-- Header -->
    <header id="header">
      <a href="index.html">
        <h1>Nicolás Pavón</h1>
      </a>
      <nav>
        <a href="ABO-eng.html">ENG</a>
        <a href="#menu">Menú</a>
      </nav>
    </header>

    <!-- Menu -->
    <nav id="menu"></nav>

    <!-- Wrapper -->
    <section id="wrapper">
      <header id="ABO-header">
        <div class="inner">
          <h2>Clasificando imagenes de Amazon</h2>
        </div>
      </header>

      <!-- Content -->
      <div class="wrapper">
        <div class="inner">
          <section>
            <h3 class="orange_title major">Introducción</h3>
            <p>Esto es la continuación del proyecto sobre redes neuronales y clasificacion de imágenes de la
              materia "Inteligencia Artificial 2" dictada por Juan Kurucz y Ernesto Ocampo en la Universidad Católica
              del
              Uruguay. Trata de la preparación de un dataset problemático y realista de productos de
              e-commerce, para luego entrenar una red neuronal utilizando transfer learning, entre otras técnicas
              interesantes.</p>
            <p>En este blog explico en detalle los problemas y soluciones encontrados en el camino, por lo que su
              version completa puede ser muy extensa (15 min de lectura). Si cuentas con poco tiempo puedes cambiar a la
              version resumida (8 min)
              haciendo click en el siguiente botón:
            </p>

            <button id="complete-btn" onclick="toggleVersion()" style="margin-top: -20px; margin-bottom: 60px">Ver
              version resumida</button>
            <button id="resumed-btn" onclick="toggleVersion()" style="margin-top: -20px; margin-bottom: 60px">Ver
              version original (recomendado)</button>
          </section>
          <section class="extended">
            <h3 class="orange_title major">El problema</h3>
            <p>
              En los últimos 20 años, el comercio electrónico ha crecido exponencialmente. Basta con observar el poder y
              el tamaño de sitios como Amazon, Alibaba o incluso Mercado Libre para darse cuenta de la importancia que
              tienen hoy en día. A partir de esto, podemos concluir que, si hay algo que tienen estos gigantes de la
              informática, es una enorme cantidad de datos, entre ellos, muchas imágenes. Pese a esto, los datos no
              sirven de nada si no se pueden interpretar y trabajar, por lo que es útil poder clasificarlos para darles
              un uso adecuado y sacarles todo el provecho posible.
            </p>
            <p>Sin embargo, mi objetivo no es resolver ese gran desafío global. Siendo realistas,
              estoy más interesado en mostrar mis habilidades en machine learning 😎. Así que aquí va!
            </p>

            <h3 class="orange_title major">Los datos</h3>
            <p>
              Entre todos los conjuntos de datos disponibles, nos topamos con uno <i>interesante</i>, el <a
                class="a-custom" href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Amazon Berkeley
                Objects (ABO)
                Dataset.</a> Este conjunto de datos nos proporciona imágenes de aproximadamente 147,000 productos de
              Amazon, con su correspondiente metadata, que incluye su categoría, color, palabras clave, marca, nombre,
              modelo, entre otros. Además, ofrece renders en 3D y algunos otros detalles interesantes. Si bien aún no
              hemos explorado los datos en profundidad para determinar su pureza, apreciamos el hecho de que provienen
              de Amazon, lo que hace que las imágenes sean ideales para este problema.
            </p>

            <h3 class="orange_title major">La tecnología</h3>
            <p>
              Si bien hoy en día suelen utilizarse los transformers para este tipo de problemas, en este caso
              utilizaremos redes convolucionales, partiendo de un modelo preentrenado como Inception-v3 y aplicando
              transfer learning, donde eliminaremos las capas superiores de clasificación y añadiremos nuevas capas
              especializadas para esta tarea. Por último, aplicaremos fine-tuning para mejorar el rendimiento. El código
              fue desarrollado <a class="a-custom" target="_blank"
                href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj?usp=sharing">en Google
                Colab.</a>
            </p>

            <h3 class="orange_title major">El objetivo</h3>
            <p>
              Nuestro objetivo es clasificar el producto en la imagen, asumiendo que la imagen corresponde a un producto
              de comercio electrónico. Tomando esto en cuenta y analizando el conjunto de datos, observamos la propiedad
              <i class="variable">product_type</i>, la cual tiene alrededor de 574 clases que varían en nivel de
              precisión, desde "RING" hasta "BISS" (Business, Industrial, and Scientific Supplies). Partiremos de esta
              propiedad del conjunto de datos para entrenar nuestro modelo.
            </p>
          </section>
          <section class="resumed">
            <h3 class="orange_title major">Resumen</h3>
            <p>
              El comercio electrónico ha crecido enormemente en las últimas décadas, impulsado por gigantes como Amazon
              y Alibaba. Con este crecimiento, la cantidad de datos también ha aumentado, especialmente en forma de
              imágenes de productos. Para aprovechar estos datos, es fundamental poder clasificarlos adecuadamente.
              Aunque, siendo realistas, en este caso no estoy aquí para salvar el comercio global, sino para mostrar lo
              que sé hacer en machine learning 😅. Vamos a trabajar con el <a class="a-custom"
                href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Amazon Berkeley
                Objects (ABO)
                Dataset.</a>, que contiene
              imágenes y metadata de más de 147,000 productos de Amazon.
            </p>
            <p>
              Para abordar este problema de clasificación, usaremos redes convolucionales y transfer learning con un
              modelo preentrenado como Inception-v3. Eliminaremos las capas de clasificación originales y las
              reemplazaremos con capas personalizadas para mejorar el rendimiento en la tarea específica de
              clasificación de productos, ajustando el modelo con fine-tuning. El objetivo es clasificar la mayor
              cantidad de clases diferentes de productos basándonos en la propiedad product_type del conjunto de datos,
              que incluye desde
              categorías muy específicas como "RING" hasta otras más amplias como "BISS" (Business, Industrial, and
              Scientific Supplies).
            </p>
          </section>
        </div>
      </div>
      <div class="wrapper alt style4">
        <div class="inner">
          <section>
            <h2 class="white-big major">Informacion y estructura del dataset</h2>
            <h3 class="orange_title major">Estructura del dataset</h3>
            <p>
              <a class="a-custom" href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">El conjunto de
                datos</a> cuenta con
              varios archivos para descargar, de los cuales nos interesan <i class="variable">listings.tar</i> (listado
              de productos y metadata) e <i class="variable">images-small.tar</i> (catálogo de imágenes reescaladas a un
              máximo de 256 píxeles).
            </p>
            <p class="extended">El archivo <i class="variable">listings.tar</i> contiene 15 archivos
              .json, cada uno con una lista de objetos, siendo cada objeto un producto de Amazon. Utilizaremos un script
              para pasar la información relevante de estos objetos a archivos .csv, para que sean más cómodos de
              trabajar. Los objetos tienen una serie de atributos, de los cuales nos interesarán <b><i>item_id</i></b>,
              <b><i>product_type</i></b>, <b><i>main_image_id</i></b> y <b><i>other_image_id</i></b>
            </p>
            <h3 class="orange_title major">Atributos del dataset y estadisticas</h3>
            <p>
              Una vez que tenemos el archivo .csv inicial, procedemos a observar la distribución de las clases:
            </p>

            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS inicial desb.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Dataset inicial</i></p>
              </span>
            </div>

            <p class="extended">Como se ve en la imagen, el conjunto de datos está totalmente desbalanceado, con muchos
              ejemplos para
              ciertas categorías y casi ninguno en otras. Al observar los datos en detalle, vemos que hay 574
              categorías, de las cuales 460 tienen menos de 100 ejemplos. Esto es un problema, ya que necesitamos una
              buena cantidad de imágenes por categoría para poder identificar ese tipo de objetos con éxito, y solo 100
              o menos no son suficientes.</p>
            <p class="extended">
              Para lidiar con este problema, en un principio trabajaremos solo con las
              categorías que tengan más ejemplos, balanceando los mismos para evitar sesgos entre las categorías al
              momento de entrenar. Originalmente, se optó por trabajar con 170 categorías con al menos 50 ejemplos por
              categoría. Esto no dio resultado, por lo que se redujo el conjunto de datos a todas las categorías que
              tuvieran al menos 150 ejemplos, con un tope de 400. Esta fue una decisión algo arbitraria, por lo que, si
              se vuelve necesario, es posible encontrar una mejor selección de categorías y ejemplos.
            </p>

            <p class="resumed">
              El dataset está desbalanceado, con 574 categorías, de las cuales 460 tienen menos de 100 ejemplos, lo que
              dificultará identificar objetos correctamente. Para abordar esto, se seleccionarán categorías con al menos
              150 ejemplos y un máximo de 400. Esta selección fue algo arbitraria y podría mejorarse si fuera necesario.
            </p>
            <p style="margin-bottom: 10px;">Una vez realizados los filtros, podemos observar las estadísticas del
              conjunto de datos final:
            </p>

            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS min 150 max 400.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Dataset simplificado</i></p>
              </span>
            </div>

            <p>En este conjunto de datos tenemos 80 categorías, mucho mejor balanceadas que las 574 del conjunto de
              datos inicial. Esto facilitará el trabajo, ya que la red neuronal final será más fácil de entrenar y
              tendrá un promedio de ejemplos por categoría considerablemente mayor.
            </p>

            <h3 class="orange_title major" id="inspeccion">Inspección del dataset</h3>
            <p>
              En este paso, analizaremos el conjunto de datos previamente refinado en busca de posibles problemas
              evidentes, entre los cuales encontramos:
            <h5>Categorias confundibles: </h5>
            <p>Estas categorías contienen objetos muy similares entre sí. Incluso, en algunos casos, la única forma de
              diferenciarlos es leyendo el texto que tiene el producto en la etiqueta. Esto es un problema, ya que para
              la red neuronal será
              difícil aprender las diferencias.</p>
            <ul>
              <li><i class="variable">ACCESORY</i> &#8596; <i class="variable">HAT</i></li>
              <li><i class="variable">STORAGE_HOOK</i> &#8596; <i class="variable">TOOLS</i></li>
              <li><i class="variable">NUTRITIONAL_SUPLEMENT</i> &#8596; <i class="variable">VITAMINS</i> &#8596; <i
                  class="variable">HEALTH_PERSONAL_CARE</i></li>
              <li><i class="variable">LUGGAGE</i> &#8596; <i class="variable">SUIT_CASE</i></li>
              <li><i class="variable">FINERING</i> &#8596; <i class="variable">RING</i></li>
              <li><i class="variable">FINENECKLACEBRACALETANKLET</i> &#8596; <i class="variable">NECKLACE</i></li>
              <li><i class="variable">FINEEARING</i> &#8596; <i class="variable">EARRING</i></li>
            </ul>
            <p class="extended">Exceptuando los casos de 'fine x' ↔ 'x', en un principio conservaremos estas categorías
              y observaremos si
              son efectivamente problemáticas al momento de clasificar. Para los casos 'fine x', nos quedaremos con los
              que no son "fine", ya que son más abarcativos y siguen preservando la forma general.</p>

            <h5>Categorias genericas: </h5>
            <p class="extended">Estas categorías contienen objetos muy variados, por lo que será más difícil entrenar a
              la red en busca
              de patrones compartidos. Si todos los objetos de una categoría varían en forma, no existe un conjunto de
              features o patrones que los unifique, y la red no podrá categorizar eficientemente. Solo sería posible
              lograrlo si cada subgrupo de objetos en esta categoría tuviera suficientes imágenes, pero como quizás de
              400 imágenes solo 90 pertenecen a uno de estos objetos, será muy difícil de entrenar. Por esta razón,
              estas categorías serán eliminadas del conjunto de datos.
            </p>
            <p class="resumed">Estas categorías son demasiado variadas, lo que dificulta que la red encuentre patrones
              comunes. Al no
              haber suficientes imágenes por subgrupo dentro de cada categoría, la red no podrá clasificarlas
              eficientemente, por lo que serán eliminadas del dataset.</p>
            <div class="generic-cat">
              <ul>
                <li><i class="variable">HOME</i></li>
                <li><i class="variable">WIRELESS_ACCESORY</i></li>
                <li><i class="variable">ACCESORY_OR_PART_OR_SUPPLY</i></li>
                <li><i class="variable">BABY_PRODUCT</i></li>
                <li><i class="variable">COMPUTER_ADDON</i></li>
                <li><i class="variable">GROCERY</i></li>
                <li><i class="variable">SPORTING_GOODS</i></li>
                <li><i class="variable">PANTRY</i></li>
                <li><i class="variable">KITCHEN</i></li>
                <li><i class="variable">JANITORY_SUPPLY</i></li>
                <li><i class="variable">HOMEFURNITURE_AND_DECOR</i></li>
                <li><i class="variable">HARDWARE</i></li>
              </ul>

              <div class="gtr-uniform home-images"
                style="flex-direction: column; align-items: flex-start; margin-bottom: 20px; margin-left: 16px; flex: 1; min-width: 0;">
                <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Ejemplos de productos en la categoria
                  <i class="variable">HOME</i>
                </h5>
                <div style="flex-direction: row; display: flex;">
                  <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                    <div class="oth-row">
                      <span>
                        <img src="images/ABO/HOME_3.jpg" alt="Home product example 2" />
                      </span>
                      <span>
                        <img src="images/ABO/HOME_2.jpg" alt="Home product example 1" />
                      </span>
                      <span>
                        <img src="images/ABO/HOME_1.jpg" alt="Home product example 1" />
                      </span>
                    </div>
                    <p style="margin: 0px;"><i>(No se parecen en nada)</i></p>
                  </div>
                </div>
              </div>
            </div>

            </p>
            <h3 class="orange_title major">Balanceo del dataset</h3>
            <p class="extended">
              Como se comentó previamente, un posible problema es el sesgo que puede generar el desbalance de ejemplos
              al momento de entrenar una red neuronal. Si en nuestra red tenemos mil ejemplos de zapatos y cien ejemplos
              de sillones, para la red las probabilidades de recibir un zapato son 10 veces mayores que las de recibir
              un sillón. En este caso, la red podría retornar siempre "zapato", acertando la mayoría de las veces. Esto
              afectaría la clasificación de forma bastante drástica, por lo que nos interesa tener el conjunto de datos
              lo más balanceado posible.

              En nuestro caso, tenemos varias categorías con menos de 400 ejemplos, que es el número ideal que queremos
              mantener en todas las categorías. Para lograr el balance deseado, tomaremos en cuenta las "other_images"
              disponibles por cada objeto que nos provee el dataset. Estas imágenes pueden ayudarnos a completar la
              cantidad de imágenes para aquellas categorías que lo necesiten.
            </p>
            <p class="resumed">
              El desbalance de ejemplos puede generar sesgos en la red neuronal, favoreciendo categorías con más
              ejemplos. Para evitar esto, es importante balancear el dataset. Como algunas categorías tienen menos de
              400 ejemplos, utilizaremos las "other_images" disponibles para completar las imágenes necesarias.
            </p>
            <h5>Ejemplos satisfactorios</h5>
            <p>Luego de una no muy breve inspección, observamos casos satisfactorios en los que las "other_images" son
              suficientemente similares (pero no idénticas) al producto original.</p>

            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Objeto en la categoria <i
                  class="variable">SOFA</i></h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <img src="images/ABO/SOFA_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/SOFA_OTH_3.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/SOFA_OTH_1.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/SOFA_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <h5>Ejemplos problematicos</h5>
            <p>Sin embargo, también observamos imágenes que no son del producto en sí, sino de una tabla descriptiva, un
              color, o de una toma general en la que el objeto es casi indistinguible.</p>

            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Objeto en la categoria <i
                  class="variable">LEGUME</i></h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image" style="max-width: 170px;">
                  <span>
                    <img src="images/ABO/LEGUME_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/LEGUME_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_4.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/LEGUME_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>

            <div class="gtr-uniform home-images extended"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Objeto en la categoria <i
                  class="variable">RUG</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <img src="images/ABO/RUG_MAIN.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <img src="images/ABO/RUG_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <img src="images/ABO/RUG_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <img src="images/ABO/RUG_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <p class="extended">Estos casos nos perjudican. Nos interesa tener cierta variabilidad en las imágenes para
              que nuestra red
              se vuelva más robusta, pero cuando tenemos imágenes demasiado complejas, o que ni siquiera contienen el
              objeto en sí, perjudican el entrenamiento de la red, ya que esta asociará patrones erróneos a la categoría
              en cuestión. ¡Recuerda el ejemplo de <i class="variable">RUG</i>! Será un problema en el futuro.
            </p>
            <p class="extended">Para superar este problema, haremos un filtrado de las "other_images" utilizando redes
              neuronales
              preentrenadas. En este caso, utilizaremos el modelo VGG16, quitando las capas de clasificación. Esto nos
              dejará una red que solo detecta features en una imagen, pero no la clasifica.

              Con esta red, procederemos a extraer las features de la imagen principal de cada objeto (main_image) y
              luego compararemos dichas features con las de cada una de las "other_images" de este objeto, obteniendo un
              coeficiente de similitud entre ellas. Este coeficiente nos indicará qué tan similares son las
              "other_images" a la imagen principal, asignando un valor muy bajo a aquellas que no sean similares.
            </p>

            <p class="resumed">
              Estos casos complican el entrenamiento, ya que imágenes demasiado complejas o irrelevantes hacen que la
              red aprenda patrones incorrectos. Para resolverlo, usaremos el modelo VGG16, eliminando las capas de
              clasificación, para extraer y comparar las features entre la imagen principal y las "other_images". Así,
              obtendremos un coeficiente de similitud que utilizaremos para filtrar las imágenes menos útiles.
            </p>
            <p style="margin-bottom: 10px;">Aqui observamos algunos ejemplos del uso de esta técnica:</p>
            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores del objeto <i
                  class="variable">SOFA</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>

                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/SOFA_MAIN.jpg" alt="" />
                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
              display: flex;
              flex-direction: column;
              height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                    
                      margin: 0px;
                      margin-bottom: -9px;
                      font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.861</i></p>
                      <progress value="0.8615963" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_1.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.640</i></p>
                      <progress value="0.6404396" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_2.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.599</i></p>
                      <progress value="0.5996146" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_3.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.196</i></p>
                      <progress value="0.1969997" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/SOFA_OTH_4.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <div class="gtr-uniform home-images"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores del objeto <i
                  class="variable">OTTOMAN</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>

                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/OTTOMAN-MAIN.jpg" alt="" />
                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
              display: flex;
              flex-direction: column;
              height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                    
                      margin: 0px;
                      margin-bottom: -9px;
                      font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.819</i></p>
                      <progress value="0.819" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.8198348.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.689</i></p>
                      <progress value="0.689" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.6890952.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.581</i></p>
                      <progress value="0.581" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.5811923.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.192</i></p>
                      <progress value="0.19274572" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/OTTOMAN-0.19274572.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>

            <p class="extended" style="margin-bottom: 10px;">¡Genial! Vemos que funciona, sin embargo, encontramos
              algunos problemas::
            </p>
            <p class="extended">La categoría RUG nos complica un poco. Si observamos algunos ejemplos, veremos que la
              "main_image" suele
              contener la alfombra en una escena genérica, un poco "escondida". Esto causa que el coeficiente de
              similitud de las "other_images" sea muy bajo, dejando fuera muchas imágenes útiles.</p>
            <div class="gtr-uniform home-images extended"
              style="flex-direction: column; align-items: flex-start; margin-bottom: 20px">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Similarity scores del objeto <i
                  class="variable">RUG</i>
              </h5>
              <div class="generic-cat">
                <div class="col-4 home-image main-image">
                  <span>
                    <p style="display: flex;
                    margin: 0px;
                    margin-bottom: -9px;"><i>Similarity score:</i><i style="font-weight: bold; color:#ee5f0f;
                    margin-left: 5px;">1</i></p>
                    <progress value="1" max="1" class="progress-bar"></progress>
                    <img src="images/ABO/RUG-MAIN-SIM.jpg" alt="" />

                    <p style="margin: 0px;"><i>Main image</i></p>
                  </span>
                </div>
                <div class="row-4 home-image" style="align-self: center;
                display: flex;
                flex-direction: column;
                height: fit-content;">
                  <div class="oth-row">
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.273</i></p>
                      <progress value="0.27311817" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.27311817.jpg" alt="Home product example 2" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.213</i></p>
                      <progress value="0.21372926" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.21372926.jpg" alt="Home product example 1" />
                    </span>
                    <span>
                      <p style="
                      
                      margin: 0px;
                      margin-bottom: -9px; font-size: 19px; display: flex;"><i style="font-weight: bold; color:#ee5f0f;
                      margin-left: 5px;">0.187</i></p>
                      <progress value="0.18717194" max="1" class="progress-bar"></progress>
                      <img src="images/ABO/RUG-0.18717194.jpg" alt="Home product example 1" />
                    </span>
                  </div>
                  <p style="margin: 0px;"><i>Other images</i></p>
                </div>
              </div>
            </div>
            <p class="extended">Utilizaremos un threshold de 0.5 como criterio para seleccionar las "other_images",
              eligiendo aquellas
              que tengan un coeficiente de similitud mayor a este para completar las imágenes faltantes en una
              categoría. Sin embargo, en el caso de la categoría <i class="variable">RUG</i>, tomaremos un threshold de
              solo 0.2, ya que, debido a las características de las "main_images", la mayoría de las "other_images"
              quedarían fuera.
            </p>
            <p class="resumed">¡Genial! Vemos que funciona, utilizaremos un threshold de 0.5 como criterio para
              seleccionar las "other_images", eligiendo aquellas
              que tengan un coeficiente de similitud mayor a este para completar las imágenes faltantes en una
              categoría.
            </p>

            <h3 class="orange_title major">Dataset final</h3>
            <p class="resumed">
              Para finalizar, organizamos las imágenes por categoría usando los similarity scores de las "other_images"
              para completar aquellas con pocos ejemplos. El dataset resultante tiene 65 categorías y unas 25,800
              imágenes. Aunque algunas categorías no alcanzaron las 400 imágenes, al tener más de 320, se consideraron
              suficientes.
            </p>
            <p class="extended">
              Por último, para terminar de armar el conjunto de datos y poder entrenar la red neuronal, debemos
              organizar las imágenes, agrupándolas por categoría. Una vez que conocemos los similarity scores de las
              "other_images", procedemos a mover todas las "main_images" a la carpeta de su categoría y completamos
              aquellas que tengan pocos ejemplos con las "other_images" que tengan mayor similarity score.
            </p>
            <p style="margin-bottom: 10px;">
              Este es el balance del dataset resultante, mucho mejor!
            </p>
            <div class="gtr-uniform home-images" style="margin-bottom: 10px">
              <span class=" image fit" style="margin: 0px;"><img src="images/ABO/DS FINAL.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 10px;"><i>Dataset final</i></p>
              </span>
            </div>
            <p id="categorias" class="extended">
              Este dataset cuenta con 65 categorías y aproximadamente 25,800 imágenes. Reconocemos que hay categorías
              que no alcanzaron las 400 imágenes debido a la falta de un buen similarity score, pero dado que aún tienen
              una cantidad considerable (>320), simplemente lo ignoraremos.
            </p>
            <h5 style="margin-bottom: 10px;" class="extended">
              Categorias finales:
            </h5>
            <div class="categories-container extended"
              style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 40px;">
              <!-- Categorías comunes diarias -->
              <div class="category-group" style="width: 100%;">
                <i class="variable_2">COFFEE</i>
                <i class="variable_2">TEA</i>
                <i class="variable_2">BREAD</i>
                <i class="variable_2">DRINKING_CUP</i>
                <i class="variable_2">HEADPHONES</i>
                <i class="variable_2">CHARGING_ADAPTER</i>
                <i class="variable_2">SHOES</i>
                <i class="variable_2">PILLOW</i>
                <i class="variable_2">CHAIR</i>
                <i class="variable_2">WALL_ART</i>
                <i class="variable_2">LAMP</i>
                <i class="variable_2">RING</i>
                <i class="variable_2">HAT</i>
                <i class="variable_2">BACKPACK</i>
                <i class="variable_2">SUITCASE</i>
                <i class="variable_2">PLANTER</i>
                <i class="variable_2">WALLET</i>
              </div>

              <button id="show-more-btn" onclick="showMoreCategories()" style="margin-top: -20px;">...ver más</button>

              <!-- Comida y Salud -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Comida y Salud</h5>
                <i class="variable_2">LEGUME</i>
                <i class="variable_2">HERB</i>
                <i class="variable_2">HEALTH_PERSONAL_CARE</i>
                <i class="variable_2">SKIN_CLEANING_AGENT</i>
                <i class="variable_2">SKIN_MOISTURIZER</i>
                <i class="variable_2">BEAUTY</i>
                <i class="variable_2">VITAMIN</i>
                <i class="variable_2">NUTRITIONAL_SUPPLEMENT</i>
              </div>

              <!-- Muebles -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Muebles</h5>
                <i class="variable_2">SHELF</i>
                <i class="variable_2">CABINET</i>
                <i class="variable_2">DESK</i>
                <i class="variable_2">TABLE</i>
                <i class="variable_2">HEADBOARD</i>
                <i class="variable_2">BED</i>
                <i class="variable_2">OTTOMAN</i>
                <i class="variable_2">STOOL_SEATING</i>
                <i class="variable_2">SOFA</i>
              </div>

              <!-- Decoración y Ropa de Cama -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Decoración y Ropa de Cama</h5>
                <i class="variable_2">RUG</i>
                <i class="variable_2">FLAT_SHEET</i>
                <i class="variable_2">FURNITURE_COVER</i>
                <i class="variable_2">LIGHT_FIXTURE</i>
              </div>

              <!-- Accesorios y Joyas -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Accesorios y Joyas</h5>
                <i class="variable_2">NECKLACE</i>
                <i class="variable_2">EARRING</i>
                <i class="variable_2">ACCESSORY</i>
                <i class="variable_2">HANDBAG</i>
                <i class="variable_2">BOOT</i>
                <i class="variable_2">SANDAL</i>
                <i class="variable_2">PORTABLE_ELECTRONIC_DEVICE_COVER</i>
                <i class="variable_2">CELLULAR_PHONE_CASE</i>
                <i class="variable_2">SCREEN_PROTECTOR</i>
              </div>

              <!-- Artículos de Oficina y Limpieza -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Artículos de Oficina y Limpieza</h5>
                <i class="variable_2">OFFICE_PRODUCTS</i>
                <i class="variable_2">STORAGE_BINDER</i>
                <i class="variable_2">STORAGE_HOOK</i>
                <i class="variable_2">CLEANING_AGENT</i>
                <i class="variable_2">BATTERY</i>
              </div>

              <!-- Otros -->
              <div class="category-group" style="display: none; width: 100%;">
                <h5>Otros</h5>
                <i class="variable_2">AUTO_ACCESSORY</i>
                <i class="variable_2">TOOLS</i>
                <i class="variable_2">SAFETY_SUPPLY</i>
                <i class="variable_2">FOOD_SERVICE_SUPPLY</i>
                <i class="variable_2">BISS</i>
                <i class="variable_2">LIGHT_BULB</i>
                <i class="variable_2">OUTDOOR_LIVING</i>
                <i class="variable_2">PET_SUPPLIES</i>
              </div>
            </div>
          </section>
        </div>
      </div>

      <div class="wrapper style3">
        <div class="inner">
          <section>
            <h2 class="white-big major">Diseño y entrenamiento de la red neuronal</h2>
            <h3 class="orange_title major">Entrenamiento de la red</h3>
            <div class="extended">
              <p>Lograr que la red tuviera un buen rendimiento fue difícil. Como se mencionó previamente, se optó por
                aplicar transfer learning, partiendo de un modelo preentrenado sin sus top layers (las capas de
                clasificación). Este modelo se encargaría de detectar las features o características principales en las
                imágenes, y luego sobre esto
                se agregarían capas personalizadas encargadas de clasificar estas features dentro de las 65 categorías
                posibles. Por último, se aplicaría fine-tuning para optimizar el rendimiento.
              </p>
              <p>En un principio, se optó por utilizar el modelo VGG-16, agregando varias capas para la clasificación (3
                capas dense y 1 capa de dropout). Este modelo tuvo un rendimiento muy pobre.</p>
              <p>Se optó por simplificar el problema reduciendo la cantidad de categorías y, además, utilizar
                Inception-v3. Aquí se empezaron a notar mejoras, sobre todo cuando se simplificó la etapa de
                clasificación, reduciéndola a 2 dense layers, 1 dropout y 1 BatchNormalization.</p>
              <p>Luego de varias iteraciones, se lograron métricas satisfactorias. El modelo más performante solo agrega
                una capa dense de 256 unidades, acompañada de un Dropout(0.4) y una capa de data augmentation con varias
                técnicas para evitar el overfitting. Sorprendentemente, esta red tan sencilla es de las más
                performantes.
                Por esto, podemos asumir que el modelo Inception-v3 ya hace un excelente trabajo al detectar las
                features
                en una imagen, dejándonos poco trabajo para completar el modelo.</p>
              <p>Una vez que encontramos un diseño de red eficiente, continuamos con las pruebas, estudiando qué
                beneficia
                al modelo y qué lo perjudica. En la siguiente tabla se pueden observar las métricas de los distintos
                diseños experimentados: </p>
            </div>
            <div class="resumed">
              <p>Lograr un buen rendimiento fue complicado. Se utilizó transfer learning, empezando con un modelo
                preentrenado sin sus capas de clasificación, al que se le añadieron capas personalizadas para clasificar
                las 65 categorías. Inicialmente, se probó con VGG-16 y varias capas dense, pero el rendimiento fue
                pobre.
              </p>
              <p> Al simplificar el problema reduciendo categorías y utilizando Inception-v3 con una arquitectura más
                simple (2 dense layers, dropout y BatchNormalization), se notaron mejoras significativas. Finalmente, el
                modelo más eficiente solo añadió una capa dense de 256 unidades, dropout, y data augmentation,
                demostrando que Inception-v3 ya detecta muy bien las features, dejando poco trabajo adicional.
              </p>
              <p> A continuación, se muestran las métricas de los diferentes diseños experimentados:</p>
            </div>
            <div class="gtr-uniform home-images"
              style="margin-bottom: 10px; flex-direction: column; align-items: flex-start; background-color: #313345">
              <h5 style="margin: 0px; margin-left: 15px; margin-bottom: 5px;">Tabla comparativa de los modelos</h5>
              <table id="score-table" style="margin: 0px">
                <thead>
                  <tr>
                    <th>Version</th>
                    <th style="position: relative">Dense Layers <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Capas completamente conectadas, donde cada neurona está conectada a
                          todas las neuronas de la capa anterior.</span>
                      </span></th>
                    <th style="position: relative">Data <br>Augmentation <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Técnica para aumentar el tamaño del conjunto de datos mediante
                          transformaciones como rotaciones, recortes, o espejado de imágenes.</span>
                      </span></th>
                    <th style="position: relative; padding-right: 29px;">BatchNorm <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Normalización por lotes que acelera el entrenamiento y mejora la
                          estabilidad de la red.</span>
                      </span></th>
                    <th style="position: relative; padding-right: 29px;">Dropout <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Técnica para prevenir sobreajuste eliminando aleatoriamente neuronas
                          durante el entrenamiento.</span>
                      </span></th>
                    <th class="separator"></th>
                    <th style="position: relative; padding-right: 29px;">Accuracy <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Precisión del modelo, que indica la proporción de predicciones
                          correctas.</span>
                      </span></th>
                    <th style="position: relative">Loss <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Mide qué tan diferentes son las predicciones del modelo en comparación
                          con los valores reales.</span>
                      </span></th>
                    <th style="position: relative">Val Accuracy <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Precisión del modelo en el conjunto de validación, que indica el
                          rendimiento fuera del conjunto de entrenamiento.</span>
                      </span></th>
                    <th style="position: relative">Val Loss <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Mide qué tan diferentes son las predicciones del modelo en el conjunto
                          de validación para detectar sobreajuste.</span>
                      </span></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="description">v1.1</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.89</td>
                    <td class="loss">0.35</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.48</td>
                  </tr>
                  <tr>
                    <td class="description">v1.2</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">❌</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.97</td>
                    <td class="loss">0.09</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.56</td>
                  </tr>
                  <tr>
                    <td class="description">v1.3</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">❌</td>
                    <td class="boolean-cell">✅</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.91</td>
                    <td class="loss">0.26</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.46</td>
                  </tr>
                  <tr>
                    <td class="description">v1.4</td>
                    <td>1x(256)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">❌</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.95</td>
                    <td class="loss">0.15</td>
                    <td class="val_accuracy">0.86</td>
                    <td class="val_loss">0.51</td>
                  </tr>
                  <tr>
                    <td class="description">v2.1</td>
                    <td>1x(256) 1x(512)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.83</td>
                    <td class="loss">0.57</td>
                    <td class="val_accuracy">0.84</td>
                    <td class="val_loss">0.54</td>
                  </tr>
                  <tr>
                    <td class="description">v2.2</td>
                    <td>1x(512) 1x(1024)</td>
                    <td class="boolean-cell">✅</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="boolean-cell">✅x2</td>
                    <td class="separator"></td>
                    <td class="accuracy">0.85</td>
                    <td class="loss">0.47</td>
                    <td class="val_accuracy">0.84</td>
                    <td class="val_loss">0.55</td>
                  </tr>
                  <!-- Add more rows as needed -->
                </tbody>
              </table>
              <p style="margin-bottom: 0px; margin-top: 0px;"><i style="font-size: 14px">Todos los modelos utilizan
                  inception-v3 y fueron
                  entrenados con 20 epochs en el
                  entrenamiento inicial y 15 en la etapa de
                  fine-tunning</i></p>
              </span>
            </div>
            <div class="extended">
              <p> Analizando un poco estas estadísticas, podemos observar en los modelos v1.2 y v1.4 mejoras
                sustanciales en
                los valores de accuracy y loss, pero a la vez notamos valores de val_loss un poco peores, esto nos
                indica overfitting, lo cual tiene sentido. La capa de data augmentation busca hacer que nuestro modelo
                sea
                más robusto, alterando las imágenes de varias formas, como rotaciones aleatorias, cambios en el
                contraste
                o brillo, zooms aleatorios, etc. A su vez, el objetivo principal de las capas dropout es prevenir el
                overfitting, por lo que es entendible que empeore su performance.</p>

              <p>Por otra parte, observamos que el modelo v1.3, que carece de la capa BatchNormalization, tiene una
                mejora
                interesante en la performance. Si bien este tipo de capas son muy importantes y frecuentemente
                utilizadas
                en modelos de clasificación de imágenes, podemos atribuir esta baja en la performance al hecho de que se
                está utilizando en la etapa final de clasificación del modelo. Quizás sería más útil en una etapa
                intermedia de un modelo más complejo. </p>

              <p>Otro hecho interesante que podemos observar de las estadísticas es la similitud de performances entre
                modelos respecto al val_accuracy. Como podemos ver, todos los modelos tienen valores muy similares. Mi
                teoría es que esto se debe a que varias imágenes utilizadas para la validación están simplemente mal
                etiquetadas. Son casos similares a los de la categoría RUG, donde la primera imagen, además de contener
                la
                alfombra, también suele contener otros objetos como sillones, sillas, cuadros, etc.

                Teniendo esto en cuenta, podemos suponer que el modelo nunca será capaz de superar cierta performance,
                porque algunas imágenes están clasificadas bajo cierta categoría, pero contienen objetos de otra. Esto
                es
                un punto a estudiar y mejorar. </p>

              <p>
                Por último, observamos que aumentar la complejidad del modelo solo empeora la performance, lo cual es,
                en
                parte, sorprendente, pero por otro lado tiene sentido, ya que el modelo base Inception_v3 es muy bueno
                haciendo su trabajo, y posiblemente el output del mismo no pueda ser mejorado, dejándonos con la única
                tarea de clasificar las features en las x categorías de nuestro problema.
              </p>
            </div>

            <div class="resumed">
              <p>Al analizar las estadísticas, se observan mejoras en accuracy y loss en los modelos v1.2 y v1.4, pero
                un aumento en val_loss sugiere overfitting, lo cual es comprensible debido al desuso de data
                augmentation y
                dropouts para prevenirlo. En el modelo v1.3, la ausencia de BatchNormalization mejoró la performance,
                posiblemente porque esta capa es más útil en etapas intermedias de modelos complejos.
              </p>
              <p>
                Además, las similitudes en val_accuracy entre los modelos podrían deberse a errores de etiquetado en las
                imágenes de validación, lo que limita el rendimiento máximo alcanzable. Finalmente, aumentar la
                complejidad del modelo empeora su rendimiento, lo que refuerza la idea de que Inception_v3 ya realiza un
                excelente trabajo detectando features, dejando poca mejora posible.</p>
            </div>

            <h3 class="orange_title major">Estadísticas del modelo seleccionado</h3>
            <p style="margin-bottom: 20px">
              El modelo ganador fue el v1.3. <a class="a-custom" target="_blank"
                href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj?usp=sharing">En el
                Colab</a> se puede observar el código completo, explicado en detalle;
              recomiendo darle una vichada. A continuación, se presentan algunas estadísticas del modelo:
            </p>
            <div class="gtr-uniform home-images" style="margin-bottom: 25px; background-color: #313345">
              <span class=" image fit" style="margin: 0px;">
                <h5 style="margin: 0px; margin-left: 7px; margin-bottom: 15px;">Entrenamiento inicial</h5>
                <img src="images/ABO/train v1.3.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 19px;">
                  A diferencia de los otros modelos, para entrenar este se aumentó de 20 epochs a 40 para exprimir un
                  poco
                  mas de performance. El entrenamiento demoró 13 minutos aprox.
                </p>
              </span>
            </div>
            <div class="gtr-uniform home-images" style="margin-bottom: 50px; background-color: #313345">
              <span class=" image fit" style="margin: 0px;">
                <h5 style="margin: 0px; margin-left: 7px; margin-bottom: 15px;">Fine tunning</h5>
                <img src="images/ABO/fine v1.3.png" alt="" />
                <p style="margin-bottom: 0px; margin-top: 19px;">
                  De igual forma, se aumentaron los epochs en la etapa de fine-tuning a 25, aunque, como se puede
                  observar en el gráfico, a partir del epoch 10-15 se empiezan a notar signos de overfitting, con poca
                  mejora en los valores de val_accuracy y val_loss, que son los que nos interesan. El entrenamiento
                  demoró 30 minutos aprox.
                </p>
              </span>
            </div>
            <div class="extended">
              <h5>Estadísticas por categoría</h5>
              <p>En la siguiente tabla se pueden observar los valores de Precision, Recall y F1
                para cada categoria, donde
                las primeras son las mas problemáticas</p>

              <table id="score-table_2">
                <thead>
                  <tr>
                    <th>Category</th>
                    <th style="position: relative">Precision
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Precision mide cuántas de las predicciones positivas son
                          correctas.</span>
                      </span>
                    </th>
                    <th style="position: relative">Recall
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Recall mide cuántos de los positivos reales fueron identificados
                          correctamente.</span>
                      </span>
                    </th>
                    <th style="position: relative">F1-Score
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">F1 es la media armónica entre precisión y recall.</span>
                      </span>
                    </th>
                    <th style="position: relative">Support
                      <span class="tooltip-container">
                        <span class="info-icon">&#9432;</span>
                        <span class="tooltiptext">Support es el número total de ocurrencias de una clase específica en
                          el
                          conjunto de datos.</span>
                      </span>
                    </th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>HEALTH_PERSONAL_CARE</td>
                    <td>0.71</td>
                    <td>0.57</td>
                    <td>0.63</td>
                    <td>83</td>
                  </tr>
                  <tr>
                    <td>SHOES</td>
                    <td>0.80</td>
                    <td>0.68</td>
                    <td>0.74</td>
                    <td>66</td>
                  </tr>
                  <tr>
                    <td>LUGGAGE</td>
                    <td>0.80</td>
                    <td>0.67</td>
                    <td>0.73</td>
                    <td>83</td>
                  </tr>
                  <tr>
                    <td>ACCESSORY</td>
                    <td>0.85</td>
                    <td>0.70</td>
                    <td>0.77</td>
                    <td>76</td>
                  </tr>
                  <tr>
                    <td>CHAIR</td>
                    <td>0.82</td>
                    <td>0.70</td>
                    <td>0.76</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OUTDOOR_LIVING</td>
                    <td>0.81</td>
                    <td>0.69</td>
                    <td>0.75</td>
                    <td>81</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BISS</td>
                    <td>0.69</td>
                    <td>0.63</td>
                    <td>0.66</td>
                    <td>73</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BEAUTY</td>
                    <td>0.68</td>
                    <td>0.65</td>
                    <td>0.67</td>
                    <td>75</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OFFICE_PRODUCTS</td>
                    <td>0.67</td>
                    <td>0.65</td>
                    <td>0.66</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>WASTE_BAG</td>
                    <td>0.86</td>
                    <td>0.90</td>
                    <td>0.88</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>NUTRITIONAL_SUPPLEMENT</td>
                    <td>0.71</td>
                    <td>0.82</td>
                    <td>0.76</td>
                    <td>74</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SKIN_CLEANING_AGENT</td>
                    <td>0.78</td>
                    <td>0.82</td>
                    <td>0.80</td>
                    <td>93</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>AUTO_ACCESSORY</td>
                    <td>0.69</td>
                    <td>0.85</td>
                    <td>0.76</td>
                    <td>65</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>VITAMIN</td>
                    <td>0.82</td>
                    <td>0.75</td>
                    <td>0.79</td>
                    <td>85</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HOME_BED_AND_BATH</td>
                    <td>0.86</td>
                    <td>0.74</td>
                    <td>0.80</td>
                    <td>74</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SHELF</td>
                    <td>0.89</td>
                    <td>0.76</td>
                    <td>0.82</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FOOD_SERVICE_SUPPLY</td>
                    <td>0.75</td>
                    <td>0.82</td>
                    <td>0.79</td>
                    <td>78</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TOOLS</td>
                    <td>0.84</td>
                    <td>0.78</td>
                    <td>0.81</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>COFFEE</td>
                    <td>0.85</td>
                    <td>0.91</td>
                    <td>0.88</td>
                    <td>76</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>WALLET</td>
                    <td>0.83</td>
                    <td>0.93</td>
                    <td>0.88</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>NECKLACE</td>
                    <td>0.99</td>
                    <td>1.00</td>
                    <td>0.99</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BATTERY</td>
                    <td>0.96</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HANDBAG</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>75</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BACKPACK</td>
                    <td>0.82</td>
                    <td>0.97</td>
                    <td>0.89</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BREAD</td>
                    <td>0.97</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BED</td>
                    <td>0.86</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>73</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>BOOT</td>
                    <td>0.89</td>
                    <td>0.95</td>
                    <td>0.92</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CABINET</td>
                    <td>0.90</td>
                    <td>0.91</td>
                    <td>0.90</td>
                    <td>95</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CHARGING_ADAPTER</td>
                    <td>0.93</td>
                    <td>0.91</td>
                    <td>0.92</td>
                    <td>81</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>CLEANING_AGENT</td>
                    <td>0.89</td>
                    <td>0.87</td>
                    <td>0.88</td>
                    <td>82</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>DESK</td>
                    <td>0.91</td>
                    <td>0.90</td>
                    <td>0.90</td>
                    <td>79</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>DRINKING_CUP</td>
                    <td>0.92</td>
                    <td>0.95</td>
                    <td>0.94</td>
                    <td>76</td>
                  </tr>
                  <tr>
                    <td>EARRING</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FLAT_SHEET</td>
                    <td>0.91</td>
                    <td>0.91</td>
                    <td>0.91</td>
                    <td>66</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>FURNITURE_COVER</td>
                    <td>0.88</td>
                    <td>0.93</td>
                    <td>0.90</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HARDWARE_HANDLE</td>
                    <td>0.87</td>
                    <td>0.95</td>
                    <td>0.91</td>
                    <td>62</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HAT</td>
                    <td>0.86</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HEADBOARD</td>
                    <td>0.94</td>
                    <td>0.93</td>
                    <td>0.94</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HEADPHONES</td>
                    <td>0.96</td>
                    <td>0.94</td>
                    <td>0.95</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>HERB</td>
                    <td>0.99</td>
                    <td>0.94</td>
                    <td>0.96</td>
                    <td>71</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LAMP</td>
                    <td>0.89</td>
                    <td>0.94</td>
                    <td>0.92</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LEGUME</td>
                    <td>0.96</td>
                    <td>0.99</td>
                    <td>0.97</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LIGHT_BULB</td>
                    <td>0.91</td>
                    <td>0.97</td>
                    <td>0.94</td>
                    <td>62</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>LIGHT_FIXTURE</td>
                    <td>0.90</td>
                    <td>0.89</td>
                    <td>0.90</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>OTTOMAN</td>
                    <td>0.85</td>
                    <td>0.88</td>
                    <td>0.86</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PET_SUPPLIES</td>
                    <td>0.92</td>
                    <td>0.79</td>
                    <td>0.85</td>
                    <td>72</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PILLOW</td>
                    <td>0.87</td>
                    <td>0.98</td>
                    <td>0.92</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PLANTER</td>
                    <td>0.88</td>
                    <td>0.99</td>
                    <td>0.93</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>PORTABLE_ELECTRONIC_DEVICE_COVER</td>
                    <td>0.96</td>
                    <td>0.87</td>
                    <td>0.91</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>RING</td>
                    <td>1.00</td>
                    <td>0.94</td>
                    <td>0.97</td>
                    <td>88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>RUG</td>
                    <td>0.98</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>90</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SAFETY_SUPPLY</td>
                    <td>0.89</td>
                    <td>0.84</td>
                    <td>0.86</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SANDAL</td>
                    <td>0.87</td>
                    <td>0.93</td>
                    <td>0.90</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SAUTE_FRY_PAN</td>
                    <td>0.96</td>
                    <td>0.95</td>
                    <td>0.96</td>
                    <td>83</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SCREEN_PROTECTOR</td>
                    <td>0.96</td>
                    <td>0.99</td>
                    <td>0.97</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SKIN_MOISTURIZER</td>
                    <td>0.91</td>
                    <td>0.89</td>
                    <td>0.90</td>
                    <td>87</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SOFA</td>
                    <td>0.79</td>
                    <td>0.86</td>
                    <td>0.82</td>
                    <td>86</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STOOL_SEATING</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>0.96</td>
                    <td>79</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STORAGE_BINDER</td>
                    <td>0.93</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>69</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>STORAGE_HOOK</td>
                    <td>0.94</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>90</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>SUITCASE</td>
                    <td>0.87</td>
                    <td>0.95</td>
                    <td>0.91</td>
                    <td>80</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TABLE</td>
                    <td>0.82</td>
                    <td>0.79</td>
                    <td>0.80</td>
                    <td>84</td>
                  </tr>
                  <tr class="cat_stats">
                    <td>TEA</td>
                    <td>0.87</td>
                    <td>0.96</td>
                    <td>0.91</td>
                    <td>70</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Accuracy</strong></td>
                    <td colspan="4">0.88</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Macro avg</strong></td>
                    <td>0.87</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>5167</td>
                  </tr>
                  <tr class="cat_stats">
                    <td><strong>Weighted avg</strong></td>
                    <td>0.88</td>
                    <td>0.88</td>
                    <td>0.87</td>
                    <td>5167</td>
                  </tr>
                </tbody>
              </table>


              <button id="show-more-table-btn" onclick="showMoreStats()"
                style="margin-top: -20px; margin-bottom: 60px">...expandir</button>
            </div>

            <h3 class="orange_title major">Modelo en acción</h3>
            <p style="margin-bottom: 20px">Se creó un espacio en HuggingFace Spaces para tener el modelo activo
              utilizando Gradio de forma constante, para poder ser utilizado en cualquier momento. Puedes usarlo aquí
              abajo, subiendo una foto que encuentres en internet, o incluso fotografiando algo con tu dispositivo!</p>

            <div style="text-align: center;">
              <iframe allow="camera" src="https://nicolaspavon-amazon-classification.hf.space" frameborder="0"
                class="gradio"></iframe>
            </div>

            <p class="extended" style="margin-bottom: 20px; margin-top: 10px;">Aquí dejo las categorías más
              frecuentemente accesibles (en
              mi opinión), pero puedes fotografiar cualquier objeto de <a class="a-custom" href="#categorias">las 65
                categorías</a> para probar el modelo.</p>
            <p class="resumed" style="margin-bottom: 20px; margin-top: 10px;">Aquí dejo las categorías más
              frecuentemente accesibles (en
              mi opinión), pero puedes fotografiar cualquier objeto de las 65
              categorías para probar el modelo.</p>
            <div class="category-group-2 extended" style="width: 100%;">
              <i class="variable_2">COFFEE</i>
              <i class="variable_2">TEA</i>
              <i class="variable_2">BREAD</i>
              <i class="variable_2">DRINKING_CUP</i>
              <i class="variable_2">HEADPHONES</i>
              <i class="variable_2">CHARGING_ADAPTER</i>
              <i class="variable_2">SHOES</i>
              <i class="variable_2">PILLOW</i>
              <i class="variable_2">CHAIR</i>
              <i class="variable_2">WALL_ART</i>
              <i class="variable_2">LAMP</i>
              <i class="variable_2">RING</i>
              <i class="variable_2">HAT</i>
              <i class="variable_2">BACKPACK</i>
              <i class="variable_2">SUITCASE</i>
              <i class="variable_2">PLANTER</i>
              <i class="variable_2">WALLET</i>
            </div>
            <div class="resumed">
              <h5 style="margin-bottom: 10px;">
                Categorias finales:
              </h5>
              <div class="categories-container" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 40px;">
                <!-- Categorías comunes diarias -->
                <div class="category-group" style="width: 100%;">
                  <i class="variable_2">COFFEE</i>
                  <i class="variable_2">TEA</i>
                  <i class="variable_2">BREAD</i>
                  <i class="variable_2">DRINKING_CUP</i>
                  <i class="variable_2">HEADPHONES</i>
                  <i class="variable_2">CHARGING_ADAPTER</i>
                  <i class="variable_2">SHOES</i>
                  <i class="variable_2">PILLOW</i>
                  <i class="variable_2">CHAIR</i>
                  <i class="variable_2">WALL_ART</i>
                  <i class="variable_2">LAMP</i>
                  <i class="variable_2">RING</i>
                  <i class="variable_2">HAT</i>
                  <i class="variable_2">BACKPACK</i>
                  <i class="variable_2">SUITCASE</i>
                  <i class="variable_2">PLANTER</i>
                  <i class="variable_2">WALLET</i>
                </div>

                <button id="show-more-btn-2" onclick="showMoreCategories()" style="margin-top: -20px;">...ver
                  más</button>

                <!-- Comida y Salud -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Comida y Salud</h5>
                  <i class="variable_2">LEGUME</i>
                  <i class="variable_2">HERB</i>
                  <i class="variable_2">HEALTH_PERSONAL_CARE</i>
                  <i class="variable_2">SKIN_CLEANING_AGENT</i>
                  <i class="variable_2">SKIN_MOISTURIZER</i>
                  <i class="variable_2">BEAUTY</i>
                  <i class="variable_2">VITAMIN</i>
                  <i class="variable_2">NUTRITIONAL_SUPPLEMENT</i>
                </div>

                <!-- Muebles -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Muebles</h5>
                  <i class="variable_2">SHELF</i>
                  <i class="variable_2">CABINET</i>
                  <i class="variable_2">DESK</i>
                  <i class="variable_2">TABLE</i>
                  <i class="variable_2">HEADBOARD</i>
                  <i class="variable_2">BED</i>
                  <i class="variable_2">OTTOMAN</i>
                  <i class="variable_2">STOOL_SEATING</i>
                  <i class="variable_2">SOFA</i>
                </div>

                <!-- Decoración y Ropa de Cama -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Decoración y Ropa de Cama</h5>
                  <i class="variable_2">RUG</i>
                  <i class="variable_2">FLAT_SHEET</i>
                  <i class="variable_2">FURNITURE_COVER</i>
                  <i class="variable_2">LIGHT_FIXTURE</i>
                </div>

                <!-- Accesorios y Joyas -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Accesorios y Joyas</h5>
                  <i class="variable_2">NECKLACE</i>
                  <i class="variable_2">EARRING</i>
                  <i class="variable_2">ACCESSORY</i>
                  <i class="variable_2">HANDBAG</i>
                  <i class="variable_2">BOOT</i>
                  <i class="variable_2">SANDAL</i>
                  <i class="variable_2">PORTABLE_ELECTRONIC_DEVICE_COVER</i>
                  <i class="variable_2">CELLULAR_PHONE_CASE</i>
                  <i class="variable_2">SCREEN_PROTECTOR</i>
                </div>

                <!-- Artículos de Oficina y Limpieza -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Artículos de Oficina y Limpieza</h5>
                  <i class="variable_2">OFFICE_PRODUCTS</i>
                  <i class="variable_2">STORAGE_BINDER</i>
                  <i class="variable_2">STORAGE_HOOK</i>
                  <i class="variable_2">CLEANING_AGENT</i>
                  <i class="variable_2">BATTERY</i>
                </div>

                <!-- Otros -->
                <div class="category-group" style="display: none; width: 100%;">
                  <h5>Otros</h5>
                  <i class="variable_2">AUTO_ACCESSORY</i>
                  <i class="variable_2">TOOLS</i>
                  <i class="variable_2">SAFETY_SUPPLY</i>
                  <i class="variable_2">FOOD_SERVICE_SUPPLY</i>
                  <i class="variable_2">BISS</i>
                  <i class="variable_2">LIGHT_BULB</i>
                  <i class="variable_2">OUTDOOR_LIVING</i>
                  <i class="variable_2">PET_SUPPLIES</i>
                </div>
              </div>
            </div>

            <p>
              En <a class="a-custom" href="https://colab.research.google.com/drive/1uU5ySJ_FRQYYXtaBswYEyEkJcovputsj#scrollTo=WKmy4s-RAc1K&line=4&uniqifier=1
              ">el colab</a> se pueden observar las pruebas que se hicieron para validar el modelo. Por un lado se
              probaron imágenes ya vistas por el modelo en el entrenamiento, y luego se probaron imágenes nuevas
              extraidas de amazon manualmente.
            </p>
            <p>En estas pruebas, se pueden observar los problemas previamente planteados. Las categorias <i
                class="variable">SUITCASE</i> y <i class="variable">LUGGAGE</i> se suelen confundir, la categoria <i
                class="variable">RUG</i> suele salir en fotos de sillones o mesas, entre otros.</p>
          </section>
        </div>
      </div>
      <div class="wrapper alt style4">
        <div class="inner">
          <section>
            <h3 class="orange_title major">Aspectos a mejorar</h3>
            <div>
              <p>Como se observó en la etapa de análisis del dataset, una gran cantidad de imágenes de <i
                  class="variable">RUG</i> contienen otros elementos como sillones, sillas, mesas, entre otros. Esto
                perjudica a la red, provocando que imágenes de sillones, por ejemplo, sean clasificadas erróneamente
                bajo
                la categoría <i class="variable">RUG</i>. Quitar estas imágenes "genéricas" del dataset mejoraría
                notablemente la performance.</p>
              <p>En el dataset también hay muchas imágenes que muestran la textura o el color del objeto en venta, lo
                cual
                no aporta al entrenamiento de la red y probablemente sean la razon por la cual las estadisticas de loss
                y val_loss dan tan mal.</p>
              <p>La categoría <i class="variable">FURNITURE_COVER</i> es muy similar a categorías como <i
                  class="variable">SOFA</i> o <i class="variable">CHAIR</i>, causando clasificaciones erróneas.</p>
              <p>Como se comentó en la etapa de <a class="a-custom" href="#inspeccion">inspección del dataset</a>,
                existen
                varias categorías que son muy genéricas o confundibles. Como era de esperar, al probar el modelo con
                objetos de <i class="variable">SUITCASE</i> y <i class="variable">LUGGAGE</i>, suele confundirse en la
                clasificación. De igual forma ocurre con <i class="variable">NUTRITIONAL_SUPPLEMENT</i> ↔ <i
                  class="variable">VITAMINS</i> ↔ <i class="variable">HEALTH_PERSONAL_CARE</i>, las cuales suelen ser
                imágenes de frascos. En las <a class="a-custom" href="#score-table_2">estadísticas del modelo</a> se
                puede
                observar la mala performance que tienen estas categorías, debido a su similitud.</p>
              <p>Eliminar o unificar algunas de estas categorías mejoraría la mayoría de los problemas que tiene el
                modelo.</p>
            </div>
            <div class="extended">
              <h3 class="orange_title major">Cosas que no funcionaron</h3>
              <h5>Clustering de macro-categorias</h5>
              <p>En un principio, se asoció la mala performance del modelo a la gran cantidad de categorías. Por esta
                razón, se planteó la posibilidad de crear un modelo genérico encargado de clasificar dentro de 4-5
                macro-categorías, y luego aplicar otro submodelo para cada macro-categoría, encargado de identificar la
                categoría final. Para esto, incluso se utilizó una técnica de clustering similar a la utilizada en el
                balanceo del dataset, que agrupó las categorías similares para luego, a partir de estos grupos, generar
                las macro-categorías. A continuación, se puede observar el gráfico generado para la agrupación:
              </p>
              <div class="gtr-uniform home-images" style="margin-bottom: 10px">
                <span class=" image fit" style="margin: 0px;"><img src="images/ABO/class clustering.png" alt="" />
                  <p style="margin-bottom: 0px; margin-top: 10px;"><i>Agrupaciones de categorias por similitud</i></p>
                </span>
              </div>
              <p>Como se puede observar, agrupó exitosamente categorías como <i class="variable">SOFA</i>, <i
                  class="variable">RUG</i>, <i class="variable">CHAIR</i> y otras dentro de una posible macro-categoría
                <i class="variable">FURNITURE</i> (rama verde). De igual forma, ocurre con otras categorías similares.
              </p>
              <p>Si bien esta idea parecía prometedora, se observó que el modelo era capaz de trabajar exitosamente con
                las
                65 categorías, por lo que esta idea quedó obsoleta.
              </p>
              <h5>Imágenes en blanco y negro</h5>
              <p>Se intento pasar las imagenes a blanco y negro, ya que los colores no deberían hacer la diferencia
                entre
                una
                categoria u otra. sin embargo no se notó mejora, aunque quizás fue por un error de implementación.</p>
            </div>
          </section>
        </div>
      </div>
      <div id="conclusions" class="wrapper style2">
        <div class="inner">

          <section>

            <h2 class="white-big major">Conclusiones</h2>
            <p>En este proyecto logramos observar la eficiencia de las técnicas de transfer learning y
              fine-tuning, y destacamos la importancia de un análisis exhaustivo del dataset, logrando un modelo
              eficiente a pesar de
              las limitaciones encontradas. Aunque hay áreas de mejora, los
              resultados demuestran el valor de un enfoque incremental y constante experimentación para optimizar
              modelos
              de deep learning.</p>
            <p>
              Gracias a Juan Kurucz por la orientación, y a mis compañeros de proyecto por la ayuda al inicio del mismo,
              Tomás Rama,
              Matías Cabrera,
              Mauricio Gómez y
              Agustín Lorenzo

            </p>
            <p>
              Eso es todo, gracias por tu tiempo! Si tienes alguna duda o comentario déjamelo saber aquí abajo 🙏
            </p>
          </section>

        </div>
      </div>
    </section>

    <!-- Footer -->
    <section id="footer"></section>
  </div>

  <!-- Scripts -->
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/jquery.scrollex.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>
  <script src="scripts/menu.js"></script>
  <script src="scripts/footer.js"></script>

  <script>
    function applyColorToCell() {
      const table = document.getElementById('score-table');
      const rows = table.getElementsByTagName('tr');

      for (let i = 1; i < rows.length; i++) {
        const cells = rows[i].getElementsByTagName('td');

        const accuracy = parseFloat(cells[6].innerText);
        const loss = parseFloat(cells[7].innerText);
        const valAccuracy = parseFloat(cells[8].innerText);
        const valLoss = parseFloat(cells[9].innerText);

        // For accuracy and val_accuracy (higher is better)
        cells[6].style.backgroundColor = getGreenToRedGradient(accuracy, true);
        cells[8].style.backgroundColor = getGreenToRedGradient(valAccuracy, true);

        // For loss and val_loss (lower is better)
        cells[7].style.backgroundColor = getGreenToRedGradient(loss, false);
        cells[9].style.backgroundColor = getGreenToRedGradient(valLoss, false);
      }
    }

    function applyColorToCell_2() {
      const table = document.getElementById('score-table_2');
      const rows = table.getElementsByTagName('tr');
      console.log(rows.length)


      for (let i = 1; i < rows.length; i++) {
        const cells = rows[i].getElementsByTagName('td');
        console.log(cells)

        const precision = parseFloat(cells[1].innerText);
        const recall = parseFloat(cells[2].innerText);
        const f1 = parseFloat(cells[3].innerText);

        // For accuracy and val_accuracy (higher is better)
        cells[1].style.backgroundColor = getGreenToRedGradient(precision, true);
        cells[2].style.backgroundColor = getGreenToRedGradient(recall, true);
        cells[3].style.backgroundColor = getGreenToRedGradient(f1, true);
      }
    }

    function getGreenToRedGradient(value, higherIsBetter) {
      const colors = [
        { r: 0, g: 128, b: 0 },     // Green
        { r: 85, g: 170, b: 0 },    // Yellowgreen
        { r: 255, g: 255, b: 0 },   // Yellow
        { r: 255, g: 165, b: 0 },   // Orange
        { r: 255, g: 0, b: 0 },     // Red
        { r: 0, g: 0, b: 0 }        // Black
      ];

      // Adjust ratio for whether higher or lower is better
      let ratio = !higherIsBetter ? value : 1 - value;
      const step = 1 / (colors.length - 1);
      const index = Math.min(Math.floor(ratio / step), colors.length - 2);
      const color1 = colors[index];
      const color2 = colors[index + 1];

      const t = (ratio - index * step) / step;
      const r = Math.round(color1.r * (1 - t) + color2.r * t);
      const g = Math.round(color1.g * (1 - t) + color2.g * t);
      const b = Math.round(color1.b * (1 - t) + color2.b * t);

      return `rgb(${r}, ${g}, ${b}, 0.8)`; // Added opacity (0.8) to the colors
    }



    // Call the function to apply colors on page load
    applyColorToCell();
    applyColorToCell_2();
  </script>

</body>

</html>